# Synth√®se ‚Äî Analyse Num√©rique

# Synth√®se ‚Äî Analyse Num√©rique

## Chapitre 1 : Repr√©sentation en virgule flottante, stabilit√© et conditionnement

> üìö **Objectif du chapitre :** Comprendre comment les ordinateurs repr√©sentent les nombres r√©els de fa√ßon finie, quantifier les erreurs in√©vitables que cela engendre, et analyser la robustesse d'un algorithme (stabilit√©) vis-√†-vis d'un probl√®me donn√© (conditionnement).

### 1. Motivation : Pourquoi s'en soucier ?

> üí° **Id√©e cl√© :** L'arithm√©tique des ordinateurs n'est pas l'arithm√©tique des r√©els $\mathbb{R}$. Les erreurs num√©riques s'accumulent et peuvent avoir des cons√©quences d√©sastreuses.

**Exemples historiques frappants :**
- **Syst√®me Patriot (1991) :** Une erreur d'arrondi de 0.34 secondes sur le temps a d√©riv√© en une erreur de position d'un demi-kilom√®tre pour l'interception, causant la mort de 28 soldats.
- **Fus√©e Ariane 5 (1996) :** Trente secondes apr√®s le d√©collage, un d√©passement de capacit√© (overflow) sur la vitesse horizontale a pouss√© la fus√©e √† l'autodestruction. Perte : ~500 millions de dollars.

### 2. Repr√©sentation en virgule flottante

L‚Äôensemble $\mathbb{R}$ est infini et continu, or la m√©moire de l'ordinateur est finie. On repr√©sente donc les r√©els de mani√®re discr√®te en utilisant la **virgule flottante**.

#### D√©finition Math√©matique
Un nombre s'√©crit sous la forme (notation √† gauche, valeur √† droite) :

$$
\pm 0.d_1 d_2 \cdots d_t \cdot \beta^e = \pm \beta^e \sum_{i=1}^{t} \frac{d_i}{\beta^i}
$$

| Symbole | Nom | Signification |
| :---: | :--- | :--- |
| $\beta$ | Base | Base $2$ (binaire) ou base $10$ (d√©cimale) |
| $t$ | Chiffres significatifs | D√©termine la pr√©cision de la repr√©sentation |
| $d_i$ | $i$-√®me chiffre | $0 \leq d_i \leq \beta - 1$ |
| $0.d_1 \dots d_t$ | Mantisse | L'ensemble des chiffres significatifs |
| $e$ | Exposant | Limit√© par des bornes $e_{\min} \leq e \leq e_{\max}$ |

#### Repr√©sentation normalis√©e
Certains r√©els peuvent s'√©crire de plusieurs mani√®res (ex: $0.2 \cdot 10^1 = 0.02 \cdot 10^2$).
Pour √©viter cette ambigu√Øt√©, on ajoute une r√®gle : **le premier chiffre de la mantisse doit √™tre non nul**.

$$
d_1 \neq 0
$$

En base $2$, le seul chiffre non-nul est $1$. Le $1$ initial est donc toujours connu √† l'avance et n'a pas besoin d'√™tre stock√© en m√©moire !

On note $\mathbb{F}$ l'ensemble des r√©els normalis√©s :

$$
\mathbb{F} = \{ x \mid x = \pm 0.d_1 d_2 \cdots d_t \cdot \beta^e, \quad e_{\min} \leq e \leq e_{\max}, \quad d_1 \neq 0 \}
$$

> ‚ö†Ô∏è **Propri√©t√© essentielle :** Les nombres de $\mathbb{F}$ ne sont **pas uniform√©ment espac√©s**. Plus on s'√©loigne de z√©ro, plus la distance absolue entre deux nombres repr√©sentables est grande.

### 3. Erreurs d'arrondi et Standard IEEE 754

#### L'Unit√© d'Arrondi ($u$)
Puisque $\mathbb{R}$ n'est pas $\mathbb{F}$, on doit projeter chaque r√©el vers le plus proche repr√©sentant en machine. 
Soit $\text{fl}(x)$ ("float") le r√©el de $\mathbb{F}$ le plus proche de $x \in \mathbb{R}$.

**D√©monstration formelle de la limite d'erreur :**
Soit $x = \pm 0.d_1 d_2 \cdots d_t d_{t+1} \cdots \cdot \beta^e$.
Le flottant le plus proche $\text{fl}(x)$ gardera les $t$ premiers chiffres, et d√©pendra de la valeur de $d_{t+1}$ pour arrondir au sup√©rieur ou √† l'inf√©rieur.
La distance absolue maximale entre $x$ et $\text{fl}(x)$ est physiquement la moiti√© du poids du dernier chiffre significatif $t$ :
$$
|x - \text{fl}(x)| \leq \frac{1}{2} \beta^{-t} \cdot \beta^e
$$
Pour trouver l'erreur relative maximale, on divise par $|x|$. Or la valeur minimale absolue possible pour $|x|$ avec l'exposant $e$ (vu que $d_1 \geq 1$) est $0.100\dots_ \beta \cdot \beta^e = \beta^{-1} \cdot \beta^e$.
Ainsi, l'erreur relative maximale est born√©e par :
$$
\frac{|x - \text{fl}(x)|}{|x|} \leq \frac{\frac{1}{2} \beta^{e-t}}{\beta^{e-1}} = \frac{1}{2}\beta^{1-t}
$$

On appelle cette quantit√© **l'unit√© d'arrondi $u$** :
$$
\boxed{u = \frac{1}{2}\beta^{1-t}}
$$

Ceci garantit la formule fondamentale :
$$
\text{fl}(x) = x(1 + \varepsilon), \quad \text{avec } |\varepsilon| \leq u
$$

#### Le Standard IEEE 754
Le format en virgule flottante universel a deux d√©clinaisons majeures en binaire ($\beta = 2$) :

| Format | Bits globaux | Bits mantisse | Bits exposant | Unit√© d'arrondi $u$ |
| :--- | :---: | :---: | :---: | :--- |
| **Simple (single)** | 32 bits | 23 bits | 8 bits | $\approx 6.0 \times 10^{-8}$ |
| **Double (double)** | 64 bits | 52 bits | 11 bits | $\approx 1.1 \times 10^{-16}$ |

*Note: En Octave/Matlab, la double pr√©cision est utilis√©e par d√©faut.*

#### Cas marginaux (Overflow, Underflow, NaN)
- **Underflow :** Lorsque $|x| < x_{\min}$, on passe en repr√©sentation "d√©normalis√©e" pour tomber gracieusement vers 0. L'erreur relative explose.
- **Overflow :** Lorsque $|x| > x_{\max}$ (ou division par $0$), on obtient la valeur `¬±Inf` (Infini).
- **NaN (Not a Number) :** Repr√©sente une forme ind√©termin√©e comme $0/0$ ou $0 \cdot \infty$.

### 4. Mod√®le standard d'arithm√©tique

Soit $\circ$ l'op√©ration math√©matique exacte (addition, soustraction, multiplication, division) et $\circledcirc$ la m√™me op√©ration ex√©cut√©e par le processeur. Le r√©sultat d'une op√©ration machine v√©rifie toujours le mod√®le suivant :

$$
x \circledcirc y = \text{fl}(x \circ y) = (x \circ y)(1 + \varepsilon), \quad |\varepsilon| \leq u
$$

**D√©monstration de propagation (Exemple sur la multiplication) :**
Imaginons qu'on veuille multiplier $x$ et $y$, mais l'ordinateur stocke en r√©alit√© $\text{fl}(x)$ et $\text{fl}(y)$, qui poss√®dent d√©j√† des erreurs inh√©rentes ($\varepsilon_1, \varepsilon_2$). La machine ex√©cute ensuite sa multiplication $\otimes$, g√©n√©rant une 3√®me erreur d'arrondi $\varepsilon_3$.
$$
\text{fl}(x) \otimes \text{fl}(y) = \bigl( x(1+\varepsilon_1) \cdot y(1+\varepsilon_2) \bigr) (1+\varepsilon_3)
$$
$$
= (x \cdot y) (1 + \varepsilon_1)(1 + \varepsilon_2)(1 + \varepsilon_3)
$$
En d√©veloppant et en ignorant les termes d'ordre sup√©rieur tr√®s minuscules $\mathcal{O}(u^2)$, on obtient :
$$
\approx (x \cdot y) (1 + \varepsilon_1 + \varepsilon_2 + \varepsilon_3)
$$
L'op√©ration finale sur la machine produit le r√©sultat math√©matique exact, perturb√© par l'addition de **trois fois l'unit√© d'arrondi** !

#### Le ph√©nom√®ne d'Annulation Catastrophique
Si $x$ et $y$ sont des r√©els tr√®s proches ($x \approx y$) mais d√©j√† entach√©s d'une l√©g√®re erreur d'arrondi relative due √† des calculs pr√©c√©dents, **leur soustraction $x \ominus y$ va amplifier ces erreurs de fa√ßon dramatique**. C'est ce qu'on appelle "l'annulation catastrophique" (perte des chiffres significatifs majeurs). 

### 5. Algorithme vs Probl√®me : Stabilit√© et Conditionnement

L'erreur entre la sortie d'un algorithme informatique ($\hat{y}$) et la fonction math√©matique exacte qu'il doit √©valuer ($y = f(x)$) s'appelle l'**erreur directe**.

$$
\text{Erreur directe} = \hat{y} - y
$$

L'erreur directe d√©pend de deux composants fondamentaux qui se multiplient entre eux :
1. **La Stabilit√© (due √† l'Algorithme informatique)**
2. **Le Conditionnement (d√ª au Probl√®me math√©matique)**

#### 5.1. La Stabilit√© (Algorithme)
Un algorithme est "stable" si les erreurs commises √† cause de l'arrondi en machine ne sont pas pires que l'effet provoqu√© s'il y avait eu une minuscule incertitude sur les variables d'entr√©e. 

Dans cette optique, on a la notion tr√®s forte d'**erreur inverse** :
L'erreur inverse consiste √† postuler que le r√©sultat num√©rique obtenu, $\hat{y}$, est math√©matiquement la **solution exacte d'un probl√®me aux donn√©es l√©g√®rement alt√©r√©es** ($x + \Delta x$).

$$
f(x + \Delta x) = \hat{y}
$$
Un algorithme poss√®de une **stabilit√© inverse** s'il existe toujours un petit $\Delta x$ tel que :
$$
\frac{\|\Delta x\|}{\|x\|} \leq C u
$$
*(Ce concept donne l'assurance que notre r√©sultat inexact est au moins la r√©ponse parfaite √† une question presque identique).*

#### 5.2. Le Conditionnement (Probl√®me Math√©matique)
Le **conditionnement** est une valeur absolue propre au probl√®me de base, insensible √† l'algorithme choisi. Il mesure √† quel point de minuscules erreurs sur les donn√©es d'entr√©e $x$ vont s'amplifier ou se r√©duire en traversant la fonction $f$.

**D√©monstration formelle de la d√©finition de $\kappa(x)$ :**
On cherche le rapport entre l'erreur relative de la sortie et l'erreur relative de l'entr√©e pour la pire des tr√®s petites perturbations $\delta x$.
$$
\kappa(x) = \lim_{\epsilon \to 0} \sup_{\|\delta x\| \leq \epsilon \|x\|} \frac{\frac{\|f(x + \delta x) - f(x)\|}{\|f(x)\|}}{\frac{\|\delta x\|}{\|x\|}}
$$
Si la fonction $f$ est diff√©rentiable, on utilise le d√©veloppement de Taylor de premier ordre : $f(x + \delta x) \approx f(x) + f'(x)\delta x$.
La variation de la fonction $\|f(x + \delta x) - f(x)\|$ devient simplement $\|f'(x)\delta x\|$, ce qui donne $\leq \|f'(x)\| \|\delta x\|$.
En rempla√ßant cela dans la grande limite sup√©rieure, l'amplificateur du bruit d'entr√©e $\|\delta x\|$ se simplifie et nous donne la formule explicite magique :
$$
\boxed{\kappa(x) = \frac{\|f'(x)\| \cdot \|x\|}{\|f(x)\|}}
$$

*Interpr√©tation du conditionnement:*
- $\kappa(x) \approx 1$ : Le probl√®me est **bien conditionn√©**. 
- $\kappa(x) \gg 1$ : Le probl√®me est **mal conditionn√©** (toute petite variation initiale donnera un r√©sultat chaotique). 

**D√©monstration formelle de l'Annulation Catastrophique :**
√âtudions le probl√®me de la soustraction de deux nombres $f(x) = x_1 - x_2$.
Le vecteur d'entr√©e est $x = (x_1, x_2)^T$. La Jacobienne $f'(x) = [1, -1]$.
Sa norme (par exemple avec la norme matricielle 1) est $\|f'(x)\|_1 = 1 + |-1| = 2$.
En appliquant la formule prouv√©e juste au dessus :
$$
\kappa(\text{soustraction}) = \frac{\| [1, -1] \| \cdot \|x\|}{|x_1 - x_2|} = \frac{2 \cdot \|x\|}{|x_1 - x_2|}
$$
**Analyse math√©matique :** Si $x_1$ et $x_2$ ont presque la m√™me valeur ($x_1 \approx x_2$), le d√©nominateur $|x_1 - x_2|$ tend de force vers $0$. Un d√©nominateur qui tend vers z√©ro force la fraction $\kappa \to \infty$. 
Ceci prouve math√©matiquement que la soustraction de nombres proches est le probl√®me le plus fondamentalement mal conditionn√© de l'informatique.

---

## Chapitre 2 : Syst√®mes d'√©quations lin√©aires : m√©thodes directes

> üìö **Objectif du chapitre :** Utiliser les ordinateurs pour r√©soudre efficacement et surtout *de mani√®re stable* de larges syst√®mes d'√©quations lin√©aires sous forme matricielle $Ax = b$.

### 1. G√©n√©ralit√©s et le Conditionnement d'une Matrice

Un syst√®me lin√©aire peut s'√©crire sous forme vectorielle/matricielle compacte :

$$
Ax = b
$$

Avec $A$ une matrice $m \times n$, $x$ le vecteur des inconnues et $b$ le vecteur solution. Les m√©thodes directes de base (ce chapitre) se focalisent sur les syst√®mes **carr√©s (m = n) r√©guliers, c'est-√†-dire inversibles (d√©terminant non nul)**.

#### Le conditionnement du syst√®me lin√©aire $\kappa(A)$
De la m√™me mani√®re qu'une fonction au Chapitre 1 poss√®de un conditionnement, une **matrice poss√®de un conditionnement** face aux erreurs d'arrondis des donn√©es initiales.

**D√©monstration de la borne du conditionnement matriciel :**
Si on a une erreur pure num√©rique $\delta b$ sur le vecteur solution, alors le syst√®me r√©solu par la machine trouve $\hat{x} = x + \delta x$.
$$ A(x + \delta x) = b + \delta b \implies A \delta x = \delta b \implies \delta x = A^{-1} \delta b $$
En prenant les normes vectorielles de l'√©quation ci-dessus, et en utilisant les propri√©t√©s des normes ($\|M v\| \leq \|M\|\|v\|$) :
$$ \|\delta x\| \leq \|A^{-1}\| \|\delta b\| \quad \text{ (√âq. 1)} $$
Et on sait aussi gr√¢ce √† l'√©quation nominale $Ax = b$ que $\|b\| \leq \|A\| \|x\|$, ce qui implique que :
$$ \frac{1}{\|x\|} \leq \frac{\|A\|}{\|b\|} \quad \text{ (√âq. 2)} $$
Si on multiplie nos deux in√©quations (√âq. 1 et √âq. 2) membre √† membre pour former l'erreur relative c√¥t√© gauche :
$$
\frac{\|\delta x\|}{\|x\|} \leq \left( \|A^{-1}\| \|A\| \right) \frac{\|\delta b\|}{\|b\|}
$$

Cette formule relie le pire cas d'amplification d'erreur au produit de la norme de la matrice par la norme de son inverse. L'amplification fatale des impr√©cisions des donn√©es initiales d√©pendra donc **exclusivement des propri√©t√©s singuli√®res de la matrice $A$**.

On isole cette valeur sous le nom officiel de **Conditionnement d'une matrice** :
$$
\kappa(A) = \|A^{-1}\| \cdot \|A\|
$$

Si $\kappa(A) \approx 1$, le syst√®me est **bien conditionn√©**.
Si $\kappa(A) \gg 1$, le syst√®me est **mal conditionn√©** (exemple: la c√©l√®bre matrice de Hilbert).

### 2. Normes Vectorielles et Matricielles

Pour quantifier la "taille" des erreurs ou d'une matrice, on utilise des normes.

#### Normes Vectorielles (sur un vecteur $v$)
- **Norme 1 (Manhattan) :** $\|v\|_1 = \sum_{i=1}^n |v_i|$ *(Somme des valeurs absolues)*
- **Norme 2 (Euclidienne) :** $\|v\|_2 = \sqrt{\sum_{i=1}^n |v_i|^2}$ *(Distance g√©om√©trique ordinaire)*
- **Norme infinie (Max) :** $\|v\|_\infty = \max_{i=1 \dots n} |v_i|$ *(La plus grande composante gouverne)*

#### Normes Matricielles Induites
Une norme matricielle $\|A\|$ d√©coule des normes vectorielles, et repr√©sente "l'allongement maximum" que la matrice peut faire subir √† n'importe quel vecteur de longueur $1$ :

$$
\|A\| = \max_{v \neq 0} \frac{\|Av\|}{\|v\|}
$$

### 3. Les Mauvaises M√©thodes Directes
Avant d'√©tudier la Factorisation LU, tordons le cou √† deux tr√®s mauvaises id√©es informatiques :

1. **La m√©thode de Cramer ($x_i = \det(A_i) / \det(A)$)** : Pour un syst√®me √† $n$ inconnues, calculer ces d√©terminants co√ªte au minimum $\approx n!$ op√©rations (une complexit√© factorielle !). Un syst√®me $50 \times 50$ prendrait plus que l'√¢ge de l'univers.
2. **L'inversion matricielle brute ($x = A^{-1}b$) :** Calculer rigoureusement toute la matrice inverse $A^{-1}$ avant de multiplier par $b$ est environ $2.3 \times$ plus lent que l'algorithme "d'√©limination" qui m√®ne √† la solution de fa√ßon asym√©trique. (L'utilisation de la commande `inv(A)*b` est √† proscrire : on utilise la division gauche matricielle `A\b`).

### 4. R√©solution de r√©seaux triangulaires

R√©soudre un syst√®me dont la matrice est **uniquement construite par un triangle** est rapide, gratuit, et hyper stable.

Par exemple, un syst√®me triangulaire inf√©rieur $L$ ("Lower") :

$$
\begin{pmatrix} a_{11} & 0 & 0 \\ a_{21} & a_{22} & 0 \\ a_{31} & a_{32} & a_{33} \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} b_1 \\ b_2 \\ b_3 \end{pmatrix}
$$

Cette √©quation se r√©sout "en cascade √† l'endroit" (*forward-substitution*) ligne par ligne, avec une complexit√© d'√† peine $n^2$ op√©rations (flops), et sans aucune instabilit√© num√©rique, car on √©vite les grandes soustractions de masses. De m√™me, une matrice triangulaire sup√©rieure $U$ ("Upper") se r√©soudra par remont√©e (*backward-substitution*).

**L'id√©e de g√©nie de l'Analyse Num√©rique :** "Transformons tous les syst√®mes complexes et intriqu√©s (matrices pleines) en cascades de simples triangles."

### 5. L'√âlimination de Gauss et la Factorisation LU

**D√©monstration Algorithmique et Th√©or√®me :**
Prenons la matrice A :
$$
A = \begin{pmatrix} a_{11} & a_{12} & a_{1n} \\ a_{21} & a_{22} & a_{2n} \\ a_{n1} & a_{n2} & a_{nn} \end{pmatrix}
$$

**√âtape 1 :** On veut mettre des z√©ros dans la 1√®re colonne en dessous de $a_{11}$.
On multiplie A par une matrice d'√©limination $L_1$ :
$$
L_1 = \begin{pmatrix} 1 & 0 & 0 \\ -\frac{a_{21}}{a_{11}} & 1 & 0 \\ -\frac{a_{n1}}{a_{11}} & 0 & 1 \end{pmatrix}
\implies L_1 A = \begin{pmatrix} a_{11} & a_{12} & a_{1n} \\ 0 & a^{(2)}_{22} & a^{(2)}_{2n} \\ 0 & a^{(2)}_{n2} & a^{(2)}_{nn} \end{pmatrix}
$$
(O√π chaque fraction $-\frac{a_{i1}}{a_{11}}$ est le multiplicateur de ligne $k$).

**√âtape k :** On reproduit cela $(n-1)$ fois avec $L_2, L_3 \dots L_{n-1}$.
√Ä la fin, la partie inf√©rieure est remplie de z√©ros. Ce qu'il reste est une matrice purement triangulaire sup√©rieure qu'on appelle $U$.
$$
L_{n-1} \dots L_2 L_1 A = U
$$

**Le g√©nie du Th√©or√®me :** Si on rassemble tous ces modificateurs en les inversant du c√¥t√© du U, l'inverse de la matrice $(L_{n-1} \dots L_1)$ donne la prestigieuse matrice $L$.
Miraculeusement, $L$ est triangulaire inf√©rieure ET tous ses coefficients ne sont **rien d'autre que les coefficients multiplicateurs $l_{ik} = \frac{a_{ik}}{a_{kk}}$ bruts, rang√©s pile √† leur place !**
$$
L = (L_{n-1} \dots L_1)^{-1} = \begin{pmatrix} 1 & 0 & 0 \\ \frac{a_{21}}{a_{11}} & 1 & 0 \\ \frac{a_{n1}}{a_{11}} & \frac{a^{(2)}_{n2}}{a^{(2)}_{22}} & 1 \end{pmatrix}
$$

D'o√π l'incontournable **Th√©or√®me de la Factorisation LU** : Toute matrice r√©guli√®re peut √™tre scind√©e algorithmiquement en :
$$
\boxed{A = LU}
$$
O√π $L$ rassemble l'historique des multiplicateurs avec des $1$ sur la diagonale, et $U$ contient l'√©tat final des pivots.

Cette factorisation demande une puissance de calcul de :

$$
\text{Co√ªt de Factorisation LU} \approx \frac{2}{3}n^3 \text{ flops}
$$

*(C'est une complexit√© polynomiale $O(n^3)$ ce qui est massivement meilleur que $n!$).*

#### La R√©volution algorithmique de la s√©paration LU
Le v√©ritable int√©r√™t informatique d'imprimer la matrice "$L$" (au lieu d'effectuer le pivot de Gauss et d'oublier toutes les √©tapes) intervient si notre second membre $b$ change soudainement.

Pour r√©soudre n'importe qu'elle nouvelle √©quation $Ax = b'$, sachant qu'on a d√©j√† souffert pour extraire $A = LU$ (la d√©construction qui co√ªte le $\frac{2}{3}n^3$) :

$$
L \cdot U \cdot x = b'
$$

On s'y prend en deux temps. D'abord on pose la variable interne $(Ux) = y$ :
1. **√âtape de descente** : R√©soudre $Ly = b'$ (Cascade avant, co√ªte $n^2$ flops).
2. **√âtape de remont√©e** : R√©soudre $Ux = y$ (Cascade arri√®re, co√ªte $n^2$ flops).

Gr√¢ce √† cet enregistrement m√©moire, re-r√©soudre l'√©quation avec des variables externes alternatives devient ridiculeusement instantan√© (du $\sim O(n^2)$ !).

### 6. Le probl√®me de l'instabilit√© et la correction (Factorisation $PA=LU$)

> ‚ö†Ô∏è **Constat d'√©chec :** La factorisation na√Øve $A = LU$ n'est **pas stable**.

**D√©monstration du crash avec un petit pivot :**
Prenons la matrice :
$$ A = \begin{pmatrix} 10^{-20} & 1 \\ 1 & 1 \end{pmatrix} \quad (Ax=b \text{ avec } b=(1,2)^T) $$
Cette matrice a un $\kappa(A) \approx 2$ (Donc parfaite pour la machine a priori).
Cependant, l'algorithme $LU$ force la division par le pivot $(1, 1)$. Donc le multiplicateur $l_{21} = \frac{1}{10^{-20}} = 10^{20}$.
La matrice $U$ sera :
$$ U = \begin{pmatrix} 10^{-20} & 1 \\ 0 & 1 - 10^{20} \end{pmatrix} = \begin{pmatrix} 10^{-20} & 1 \\ 0 & -10^{20} \end{pmatrix} $$
 *(Car $1 \ominus 10^{20} = -10^{20}$ √† cause du nombre limit√© de bits de la mantisse, le "1" a √©t√© d√©truit et est tomb√© dans le n√©ant : Annulation catastrophique !)*
L'ordinateur vient de rater lamentablement le calcul en interne.

#### Le Pivotage Partiel
La parade : On choisit la ligne poss√©dant la **valeur absolue maximale** comme pivot en la permutant. 
Pour l'ordinateur, √©changer la ligne $1$ et $2$ correspond √† pr√©-multiplier l'√©quation $Ax=b$ par une matrice identit√© permut√©e $P$ :
$$
P(Ax) = P(b) \implies \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} \begin{pmatrix} 10^{-20} & 1 \\ 1 & 1 \end{pmatrix} x = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \end{pmatrix}
$$
La nouvelle matrice √† factoriser devient $\begin{pmatrix} 1 & 1 \\ 10^{-20} & 1 \end{pmatrix}$.
Maintenant, le pivot en $(1,1)$ est $1$. Le multiplicateur est $l_{21} = \frac{10^{-20}}{1} = 10^{-20}$. **Aucune erreur d'arrondi ou gonflement math√©matique infernal n'a lieu**. La stabilit√© est parfaite.

L'algorithme moderne d√©finitif est **Factorisation LU avec Pivotage ($PA = LU$)** :
$$
\boxed{PA = LU}
$$
O√π $P$ est une matrice de rep√©rage de lignes. **Le calcul $PA=LU$ est garanti inconditionnellement stable algorithmiquement.**

### 7. Applications annexes de la force LU

Le but de tout syst√®me matriciel n'est pas uniquement de trouver le vecteur $x$.
Les facteurs purs $L, U$ et $P$ ont d√©voil√© tous les secrets structurels profonds de la trame.

#### Calcul d'un D√©terminant lourd
Si on se rappelle qu'un d√©terminant matriciel se multiplie, alors :
$$ \det(P) \det(A) = \det(L) \det(U) $$
√âtant donn√© que $L$ et $U$ sont des matrices "triangulaires", leur d√©terminant s'√©l√®ve √† la simple... multiplication de toute leur propre diagonale ! Puisque la diagonale de $L$ ne contient que des $1$ (c'est forc√© par le design bas niveau de l'informatique), $\det(L) = 1$. Et le d√©terminant de $P$ est simplement $(-1)^p$ avec $p$ le nombre de lignes interchang√©es.

Le v√©ritable d√©terminant d'un monstre abstrait multidimensionnel $A$ est donc :
$$
\det(A) = (-1)^p \cdot u_{11} \cdot u_{22} \cdots u_{nn}
$$
*(En calculant la factorisation de co√ªt raisonnable, la vraie machine √† calculer Octave d√©livre les d√©terminants de mani√®re infiniment plus √©l√©gante et performante que la technique alg√©brique scolaire usuelle).*

#### Calcul profond de l'Inversion ($A^{-1}$)
L'inverse s'√©crit formellement en recherchant la collection des colonnes vectorielles $x_i$ r√©pondant √† l'ind√©pendance de la base canonnique : $A x_i = e_i$.
On ex√©cute l'algorithme de factorisation en cascade sur $PA=LU$ pour toutes les colonnes unitaires, co√ªtant au global un impressionnant total brut de : $\frac{8}{3}n^3$ flops en pur temps machine.

---

## Chapitre 3 : Factorisation QR et syst√®mes surd√©termin√©s

> üìö **Objectif du chapitre :**  Apprendre √† r√©soudre des syst√®mes sans solution exacte (Moindres Carr√©s), et d√©couvrir une factorisation alternative ($QR$) bas√©e sur des transformations g√©om√©triques orthogonales, plus lente mais vitale pour l'analyse spectrale et les statistiques multidimensionnelles.

### 1. La Factorisation QR : G√©n√©ralit√©s

La factorisation $QR$ est l'autre grande d√©composition matricielle de l'Analyse Num√©rique.
Contrairement √† $LU$ (qui scinde une matrice en deux triangles), $QR$ prend une matrice $A$ de dimension rectangulaire $m \times n$ (souvent plus "haute" que "large", avec $m \ge n$) et la scinde en :

$$
A = QR
$$

O√π :
- **$Q$ est une matrice Orthogonale** (dimension $m \times m$ pour la forme pleine). Ses colonnes forment une base de vecteurs superposables de longueur 1 √† $90^{\circ}$ les uns des autres. 
  **Propri√©t√© supr√™me :** L'inverse d'une matrice orthogonale est simplement sa transpos√©e : $Q^{-1} = Q^T \implies Q^T Q = I$.
- **$R$ est une matrice Trap√©zo√Ødale Sup√©rieure**. Les √©l√©ments en dessous de sa diagonale sont compl√®tement z√©rot√©s. Puisque $m \ge n$, les $m-n$ derni√®res lignes de $R$ sont physiquement constitu√©es uniquement de z√©ros (vide complet).

*(Pour des raisons informatiques d'√©conomie de m√©moire, MATLAB/Octave calculent souvent la factorisation QR **r√©duite** : $\hat{Q}\hat{R} = A$. On arrache la coquille inutile des z√©ros inf√©rieurs de l'√©quation).*

### 2. La Magie de la matrice de Householder ($H$)

Il existe plusieurs algorithmes pour fabriquer le $Q$ et le $R$ (Gram-Schmidt, Rotations de Givens...). Le standard absolu, stable et rapide s'appelle l'algorithme par **Transformations de Householder**.

**Le principe :** De la m√™me mani√®re que Gauss √©crasait les parties inf√©rieures des vecteurs colonnes en les multipliant par $L_1, L_2\dots$ , Householder les √©crase en les multipliant par $Q_1, Q_2\dots$, mais tout en gardant une isom√©trie parfaite (sans d√©former l'espace vectoriel, en faisant des "miroirs" multidimensionnels).

**D√©monstration de la matrice miroir de Householder :**
Householder construit un "reflet g√©om√©trique" permettant de basculer n'importe quel vecteur pointant vers le vide sur l'axe standard d√©sir√© en utilisant un vecteur miroir $v$.
La formule de la transformation $H$ associ√©e √† un vecteur de rebond $v$ est :
$$
\boxed{H = I - 2 \frac{vv^T}{\|v\|^2_2}}
$$

Cette matrice poss√®de des propri√©t√©s alg√©briques miracles :
1. **Elle est Sym√©trique ($H^T = H$) :**
   $(I - 2 \frac{vv^T}{\|v\|^2_2})^T = I^T - 2 \frac{(vv^T)^T}{\|v\|^2_2}$.
   Or $(vv^T)^T = (v^T)^T (v)^T = vv^T$.  La matrice est fondamentalement sym√©trique !
2. **Elle est Orthogonale ($H^T H = I$) ce qui implique qu'elle est sa propre inverse ($H^2 = I$) :**
   Multiplions : $HH$
   $$ HH = (I - 2 \frac{vv^T}{\|v\|^2_2}) (I - 2 \frac{vv^T}{\|v\|^2_2}) = I - 4 \frac{vv^T}{\|v\|^2_2} + 4 \frac{(vv^T)(vv^T)}{(\|v\|^2_2)^2} $$
   Le terme de droite $(vv^T)(vv^T)$ contient √† l'int√©rieur un $(v^Tv)$ scalaire qui n'est autre que la d√©finition de la norme euclidienne au carr√© $\|v\|^2_2$. Il va donc purement s'annuler avec le bas de la fraction :
   $$ = I - 4 \frac{vv^T}{\|v\|^2_2} + 4 \frac{v (\|v\|^2_2) v^T}{\|v\|^4_2} = I - \text{truc} + \text{truc} = I $$

**L'algorithme de factorisation :** 
Pour forcer des z√©ros en dessous du pivot courant de la colonne vectorielle $x$, l'ordinateur s√©lectionne le vecteur de rebond parfait :
$$ v = x \pm \|x\|_2 e_1 $$
*(En pratique sur un ordinateur, le bit de signe `sign(x1)` dictera le choix du $\pm$ pour √©viter √† tout prix une Annulation Catastrophique lors de l'op√©ration sur la mantisse).*

On attaque alors par la gauche $H_1 A$, puis $H_2 (H_1 A) \dots$ co√ªtant pour une factorisation compl√®te environ $\sim 2n^2(m - \frac{n}{3})$ flops machine.

### 3. Interlude Alg√©brique : Normes et Sym√©tries Absolues
Avant de r√©soudre le myst√®re des syst√®mes impossibles, il faut solidifier deux propri√©t√©s universelles en alg√®bre :

- **Propri√©t√© 1 :** Pour une matrice sym√©trique $A$, $\|A\|_2$ correspond directement √† la plus grande valeur propre (en valeur absolue) de son spectre spectral $\max |\lambda_i|$.
- **Propri√©t√© 2 :** L'accumulation carr√©e $\|A^T A\|_2$ se r√©duit en norme √©quivalente √† $\|A\|^2_2$. 

### 4. G√©rer l'Impossible : Les Syst√®mes Surd√©termin√©s

Dans la vraie vie (Ing√©nierie, statistiques, capteurs de Data Science), on a souvent **beaucoup plus d'√©quations** de prise de donn√©es que de variables pures dans notre mod√®le fonctionnel. Le syst√®me est un rectangle tr√®s haut : $m \gg n$. 

$$
Ax = b
$$
Dans la vaste majorit√© des cas, ce syst√®me avec trop de contraintes n'a **aucune solution stricte et exacte**. Les points ne s'alignent pas chimiquement.

L'objectif en calcul num√©rique est de s'approcher au maximum d'une issue acceptable. 
On d√©finit formellement l'√©cart √† la perfection (le **r√©sidu**) par le vecteur d'erreur : $r = b - Ax$.
L'approche choisie se nomme la r√©solution aux **Moindres Carr√©s** (Minimisation globale de la variance euclidienne).

$$
\min_x \|b - Ax\|_2
$$

### 5. La solution formelle des √âquations Normales (La Jacobienne)

**D√©monstration formelle par annulation des pentes g√©od√©siques :**
On cherche virtuellement le point vectoriel $x$ le plus bas dans la cuvette de l'erreur carr√©e $f(x) = \|b - Ax\|^2_2$. Le point critique minimum est l'unique endroit o√π le gradient multidimensionnel est nul ($\nabla f(x) = 0$).

En alg√®bre matricielle :
$$ f(x) = (b-Ax)^T(b-Ax) = b^Tb - x^TA^Tb - b^TAx + x^TA^TAx $$
On trouve la d√©riv√©e par rapport au vecteur $x$ (r√®gle des tenseurs analytiques) :
$$ \nabla f(x) = 0 - 2A^Tb + 2A^TAx $$
En √©galisant ce gradient diff√©rentiel purement √† $0$, on d√©couvre le Saint Graal des statistiques, condition de l'optimum divin :
$$
\boxed{A^T A x = A^T b}
$$
C'est le syst√®me des **√âquations Normales**. 

*(Note : Dans ces circonstances strictes, ce rectangle $(A^T A)$ d'origine devient un bloc super sym√©trique carr√© de dimension pure $(n \times n)$, positif d√©fini. Sa robustesse permet l'usage inconditionnel de Factorisations ultra-stables comme Cholesky ou $PA=LU$ !)*

#### Le Tenseur Pseudo-inverse ($A^{\dagger}$)
Dans le cas surd√©termin√©, on ne peut pas techniquement parler d'inversion ($A^{-1}$ n'existe pas pour un rectangle !). On va devoir "bricoler" une matrice de projection rectangulaire √† gauche.
Si je r√©sous l'√©quation de l'optimum $x = (A^T A)^{-1} A^T b$, j'emballe la formule dans le package math√©matique du **Pseudo-inverse de Moore-Penrose** not√© $A^{\dagger}$ ("A dague").

$$ A^{\dagger} = (A^T A)^{-1} A^T $$

### 6. Interpr√©tation G√©om√©trique de l'Improvisation ($\sin(\theta)$)

L'Analyse des donn√©es est une g√©om√©trie. Lors d'un probl√®me aux "moindres carr√©s", mon programme tente d√©sesp√©r√©ment d'approcher le vecteur de mes donn√©es ($b$) par une simple combinaison de colonnes de $A$ (l'hyperplan de l'Image de A appel√© $\text{Im}(A)$ ou Range($A$)).
Le r√©sidu $r$ est le rayon laser d'erreur vectorielle. Pour que ce rayon soit le plus "court" possible (pythagore), ce rayon doit **pointer obligatoirement √† un angle droit de $90^\circ$ de notre image plane projet√©e**.

Plus l'angle d'√©l√©vation original ($\theta$) entre mes vraies donn√©es chaotiques absolues $b$ et mon horizon plat d'approximation synth√©tique $A x_{approx}$ est grand, plus le $\sin(\theta)$ se rapproche gravement de sa limite ($1.0$). L'√©quation devient g√©om√©triquement intenable !

### 7. Le duel d√©cisif : √âquations Normales *vs* Factorisation QR

Si j'utilise mon ordinateur pour coder la r√©solution du meilleur lissage matriciel $Ax \approx b$. J'ai deux m√©thodes lourdes :

**1. La M√©thode LU sur les √âquations Normales (Force brute)**
On ordonne au processeur de g√©n√©rer physiquement $(A^TA)$ puis on l'attaque au $LU$ pivot√© de la partie 2 du cours. L'avantage absolu : calcul foudroyant !
- **Co√ªt** : L'assemblage de ce bloc de tenseur sym√©trique est $\approx mn^2$ flops, puis attaqu√© avec $\approx \frac{2}{3}n^3$ flops. Temps machine hyper domin√© par sa propre cr√©ation.

**2. La M√©thode par Algorithme QR de Householder (Chirurgie orthogonale)**
Sans jamais fabriquer $A^T A$ (qui d√©truit des informations), l'ordinateur lance ses miroirs de factorisations dessus : $A = QR \implies A^T A = (QR)^T(QR) = R^T Q^T Q R = R^T I R = R^T R$. Une fois que $Q$ est tomb√© de l'√©quation, on r√©sout simplement par un blocage triangulaire magique $Rx = Q^Tb$.
- **Co√ªt** : √ânorme p√©nalit√© $2mn^2$. Sensiblement double du temps processeur pour tout ex√©cuter.

**La trag√©die du Conditionnement $\kappa$ :**
Pourquoi utiliser la m√©thode lente $QR$ au lieu de l'algorithme bourrin des √âquations Normales ? 
La **D√©monstration du conditionnement des moindres carr√©s** r√©v√®le que la stabilit√© de tout le processus s'effondre avec la premi√®re m√©thode sous l'escalade d'une simple puissance $2$ :
$$
\kappa(\text{√âquations Normales}) = \kappa(A^T A) = \|(A^T A)^{-1}\|_2 \|A^T A\|_2 = \|A^{\dagger}\|_2^2 \|A\|^2_2 = \boxed{\kappa(A)^2}
$$

Si les donn√©es sont instables (matrice source mal-conditionnn√©e de base avec un pauvre $\kappa \approx 10^{10}$), attaquer brutalement √† la m√©thode des √©quations usuelles va **arracher les d√©cimales des pauvres flottants √† la puissance $2$ ($\approx 10^{20}$ !)** ce qui d√©truit litt√©ralement la plage de pr√©cision double de l'ordinateur ($10^{16}$). R√©sultat final en sortie ? Seulement du pur bruit blanc d√©nu√© de tout sens physique.

La force de la chirurgie $QR$ est de bloquer l'escalade : l'ordinateur sort le mod√®le **inconditionnellement stable**.
