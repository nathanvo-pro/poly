# Synth√®se ‚Äî Analyse Num√©rique

## Chapitre 1 : Repr√©sentation en virgule flottante, stabilit√© et conditionnement

> üìö **Objectif du chapitre :** Comprendre comment les ordinateurs repr√©sentent les nombres r√©els de fa√ßon finie, quantifier les erreurs in√©vitables que cela engendre, et analyser la robustesse d'un algorithme (stabilit√©) vis-√†-vis d'un probl√®me donn√© (conditionnement).

### 1. Motivation : Pourquoi s'en soucier ?

> üí° **Id√©e cl√© :** L'arithm√©tique des ordinateurs n'est pas l'arithm√©tique des r√©els $\mathbb{R}$. Les erreurs num√©riques s'accumulent et peuvent avoir des cons√©quences d√©sastreuses.

**Exemples historiques frappants :**
- **Syst√®me Patriot (1991) :** Une erreur d'arrondi de 0.34 secondes sur le temps a d√©riv√© en une erreur de position d'un demi-kilom√®tre pour l'interception, causant la mort de 28 soldats.
- **Fus√©e Ariane 5 (1996) :** Trente secondes apr√®s le d√©collage, un d√©passement de capacit√© (overflow) sur la vitesse horizontale a pouss√© la fus√©e √† l'autodestruction. Perte : ~500 millions de dollars.

### 2. Repr√©sentation en virgule flottante

L‚Äôensemble $\mathbb{R}$ est infini et continu, or la m√©moire de l'ordinateur est finie. On repr√©sente donc les r√©els de mani√®re discr√®te en utilisant la **virgule flottante**.

#### D√©finition Math√©matique
Un nombre s'√©crit sous la forme (notation √† gauche, valeur √† droite) :

$$
\pm 0.d_1 d_2 \cdots d_t \cdot \beta^e = \pm \beta^e \sum_{i=1}^{t} \frac{d_i}{\beta^i}
$$

| Symbole | Nom | Signification |
| :---: | :--- | :--- |
| $\beta$ | Base | Base $2$ (binaire) ou base $10$ (d√©cimale) |
| $t$ | Chiffres significatifs | D√©termine la pr√©cision de la repr√©sentation |
| $d_i$ | $i$-√®me chiffre | $0 \leq d_i \leq \beta - 1$ |
| $0.d_1 \dots d_t$ | Mantisse | L'ensemble des chiffres significatifs |
| $e$ | Exposant | Limit√© par des bornes $e_{\min} \leq e \leq e_{\max}$ |

#### Repr√©sentation normalis√©e
Certains r√©els peuvent s'√©crire de plusieurs mani√®res (ex: $0.2 \cdot 10^1 = 0.02 \cdot 10^2$).
Pour √©viter cette ambigu√Øt√©, on ajoute une r√®gle : **le premier chiffre de la mantisse doit √™tre non nul**.

$$
d_1 \neq 0
$$

En base $2$, le seul chiffre non-nul est $1$. Le $1$ initial est donc toujours connu √† l'avance et n'a pas besoin d'√™tre stock√© en m√©moire !

On note $\mathbb{F}$ l'ensemble des r√©els normalis√©s :

$$
\mathbb{F} = \{ x \mid x = \pm 0.d_1 d_2 \cdots d_t \cdot \beta^e, \quad e_{\min} \leq e \leq e_{\max}, \quad d_1 \neq 0 \}
$$

> ‚ö†Ô∏è **Propri√©t√© essentielle :** Les nombres de $\mathbb{F}$ ne sont **pas uniform√©ment espac√©s**. Plus on s'√©loigne de z√©ro, plus la distance absolue entre deux nombres repr√©sentables est grande.

### 3. Erreurs d'arrondi et Standard IEEE 754

#### L'Unit√© d'Arrondi ($u$)
Puisque $\mathbb{R}$ n'est pas $\mathbb{F}$, on doit projeter chaque r√©el vers le plus proche repr√©sentant en machine. 
Soit $\text{fl}(x)$ ("float") le r√©el de $\mathbb{F}$ le plus proche de $x \in \mathbb{R}$.

**D√©monstration formelle de la limite d'erreur :**
Soit $x = \pm 0.d_1 d_2 \cdots d_t d_{t+1} \cdots \cdot \beta^e$.
Le flottant le plus proche $\text{fl}(x)$ gardera les $t$ premiers chiffres, et d√©pendra de la valeur de $d_{t+1}$ pour arrondir au sup√©rieur ou √† l'inf√©rieur.
La distance absolue maximale entre $x$ et $\text{fl}(x)$ est physiquement la moiti√© du poids du dernier chiffre significatif $t$ :
$$
|x - \text{fl}(x)| \leq \frac{1}{2} \beta^{-t} \cdot \beta^e
$$
Pour trouver l'erreur relative maximale, on divise par $|x|$. Or la valeur minimale absolue possible pour $|x|$ avec l'exposant $e$ (vu que $d_1 \geq 1$) est $0.100\dots_ \beta \cdot \beta^e = \beta^{-1} \cdot \beta^e$.
Ainsi, l'erreur relative maximale est born√©e par :
$$
\frac{|x - \text{fl}(x)|}{|x|} \leq \frac{\frac{1}{2} \beta^{e-t}}{\beta^{e-1}} = \frac{1}{2}\beta^{1-t}
$$

On appelle cette quantit√© **l'unit√© d'arrondi $u$** :
$$
\boxed{u = \frac{1}{2}\beta^{1-t}}
$$

Ceci garantit la formule fondamentale :
$$
\text{fl}(x) = x(1 + \varepsilon), \quad \text{avec } |\varepsilon| \leq u
$$

#### Le Standard IEEE 754
Le format en virgule flottante universel a deux d√©clinaisons majeures en binaire ($\beta = 2$) :

| Format | Bits globaux | Bits mantisse | Bits exposant | Unit√© d'arrondi $u$ |
| :--- | :---: | :---: | :---: | :--- |
| **Simple (single)** | 32 bits | 23 bits | 8 bits | $\approx 6.0 \times 10^{-8}$ |
| **Double (double)** | 64 bits | 52 bits | 11 bits | $\approx 1.1 \times 10^{-16}$ |

*Note: En Octave/Matlab, la double pr√©cision est utilis√©e par d√©faut.*

#### Cas marginaux (Overflow, Underflow, NaN)
- **Underflow :** Lorsque $|x| < x_{\min}$, on passe en repr√©sentation "d√©normalis√©e" pour tomber gracieusement vers 0. L'erreur relative explose.
- **Overflow :** Lorsque $|x| > x_{\max}$ (ou division par $0$), on obtient la valeur `¬±Inf` (Infini).
- **NaN (Not a Number) :** Repr√©sente une forme ind√©termin√©e comme $0/0$ ou $0 \cdot \infty$.

### 4. Mod√®le standard d'arithm√©tique

Soit $\circ$ l'op√©ration math√©matique exacte (addition, soustraction, multiplication, division) et $\circledcirc$ la m√™me op√©ration ex√©cut√©e par le processeur. Le r√©sultat d'une op√©ration machine v√©rifie toujours le mod√®le suivant :

$$
x \circledcirc y = \text{fl}(x \circ y) = (x \circ y)(1 + \varepsilon), \quad |\varepsilon| \leq u
$$

**D√©monstration de propagation (Exemple sur la multiplication) :**
Imaginons qu'on veuille multiplier $x$ et $y$, mais l'ordinateur stocke en r√©alit√© $\text{fl}(x)$ et $\text{fl}(y)$, qui poss√®dent d√©j√† des erreurs inh√©rentes ($\varepsilon_1, \varepsilon_2$). La machine ex√©cute ensuite sa multiplication $\otimes$, g√©n√©rant une 3√®me erreur d'arrondi $\varepsilon_3$.
$$
\text{fl}(x) \otimes \text{fl}(y) = \bigl( x(1+\varepsilon_1) \cdot y(1+\varepsilon_2) \bigr) (1+\varepsilon_3)
$$
$$
= (x \cdot y) (1 + \varepsilon_1)(1 + \varepsilon_2)(1 + \varepsilon_3)
$$
En d√©veloppant et en ignorant les termes d'ordre sup√©rieur tr√®s minuscules $\mathcal{O}(u^2)$, on obtient :
$$
\approx (x \cdot y) (1 + \varepsilon_1 + \varepsilon_2 + \varepsilon_3)
$$
L'op√©ration finale sur la machine produit le r√©sultat math√©matique exact, perturb√© par l'addition de **trois fois l'unit√© d'arrondi** !

#### Le ph√©nom√®ne d'Annulation Catastrophique
Si $x$ et $y$ sont des r√©els tr√®s proches ($x \approx y$) mais d√©j√† entach√©s d'une l√©g√®re erreur d'arrondi relative due √† des calculs pr√©c√©dents, **leur soustraction $x \ominus y$ va amplifier ces erreurs de fa√ßon dramatique**. C'est ce qu'on appelle "l'annulation catastrophique" (perte des chiffres significatifs majeurs). 

### 5. Algorithme vs Probl√®me : Stabilit√© et Conditionnement

L'erreur entre la sortie d'un algorithme informatique ($\hat{y}$) et la fonction math√©matique exacte qu'il doit √©valuer ($y = f(x)$) s'appelle l'**erreur directe**.

$$
\text{Erreur directe} = \hat{y} - y
$$

L'erreur directe d√©pend de deux composants fondamentaux qui se multiplient entre eux :
1. **La Stabilit√© (due √† l'Algorithme informatique)**
2. **Le Conditionnement (d√ª au Probl√®me math√©matique)**

#### 5.1. La Stabilit√© (Algorithme)
Un algorithme est "stable" si les erreurs commises √† cause de l'arrondi en machine ne sont pas pires que l'effet provoqu√© s'il y avait eu une minuscule incertitude sur les variables d'entr√©e. 

Dans cette optique, on a la notion tr√®s forte d'**erreur inverse** :
L'erreur inverse consiste √† postuler que le r√©sultat num√©rique obtenu, $\hat{y}$, est math√©matiquement la **solution exacte d'un probl√®me aux donn√©es l√©g√®rement alt√©r√©es** ($x + \Delta x$).

$$
f(x + \Delta x) = \hat{y}
$$
Un algorithme poss√®de une **stabilit√© inverse** s'il existe toujours un petit $\Delta x$ tel que :
$$
\frac{\|\Delta x\|}{\|x\|} \leq C u
$$
*(Ce concept donne l'assurance que notre r√©sultat inexact est au moins la r√©ponse parfaite √† une question presque identique).*

#### 5.2. Le Conditionnement (Probl√®me Math√©matique)
Le **conditionnement** est une valeur absolue propre au probl√®me de base, insensible √† l'algorithme choisi. Il mesure √† quel point de minuscules erreurs sur les donn√©es d'entr√©e $x$ vont s'amplifier ou se r√©duire en traversant la fonction $f$.

**D√©monstration formelle de la d√©finition de $\kappa(x)$ :**
On cherche le rapport entre l'erreur relative de la sortie et l'erreur relative de l'entr√©e pour la pire des tr√®s petites perturbations $\delta x$.
$$
\kappa(x) = \lim_{\epsilon \to 0} \sup_{\|\delta x\| \leq \epsilon \|x\|} \frac{\frac{\|f(x + \delta x) - f(x)\|}{\|f(x)\|}}{\frac{\|\delta x\|}{\|x\|}}
$$
Si la fonction $f$ est diff√©rentiable, on utilise le d√©veloppement de Taylor de premier ordre : $f(x + \delta x) \approx f(x) + f'(x)\delta x$.
La variation de la fonction $\|f(x + \delta x) - f(x)\|$ devient simplement $\|f'(x)\delta x\|$, ce qui donne $\leq \|f'(x)\| \|\delta x\|$.
En rempla√ßant cela dans la grande limite sup√©rieure, l'amplificateur du bruit d'entr√©e $\|\delta x\|$ se simplifie et nous donne la formule explicite magique :
$$
\boxed{\kappa(x) = \frac{\|f'(x)\| \cdot \|x\|}{\|f(x)\|}}
$$

*Interpr√©tation du conditionnement:*
- $\kappa(x) \approx 1$ : Le probl√®me est **bien conditionn√©**. 
- $\kappa(x) \gg 1$ : Le probl√®me est **mal conditionn√©** (toute petite variation initiale donnera un r√©sultat chaotique). 

**D√©monstration formelle de l'Annulation Catastrophique :**
√âtudions le probl√®me de la soustraction de deux nombres $f(x) = x_1 - x_2$.
Le vecteur d'entr√©e est $x = (x_1, x_2)^T$. La Jacobienne $f'(x) = [1, -1]$.
Sa norme (par exemple avec la norme matricielle 1) est $\|f'(x)\|_1 = 1 + |-1| = 2$.
En appliquant la formule prouv√©e juste au dessus :
$$
\kappa(\text{soustraction}) = \frac{\| [1, -1] \| \cdot \|x\|}{|x_1 - x_2|} = \frac{2 \cdot \|x\|}{|x_1 - x_2|}
$$
**Analyse math√©matique :** Si $x_1$ et $x_2$ ont presque la m√™me valeur ($x_1 \approx x_2$), le d√©nominateur $|x_1 - x_2|$ tend de force vers $0$. Un d√©nominateur qui tend vers z√©ro force la fraction $\kappa \to \infty$. 
Ceci prouve math√©matiquement que la soustraction de nombres proches est le probl√®me le plus fondamentalement mal conditionn√© de l'informatique.

---

## Chapitre 2 : Syst√®mes d'√©quations lin√©aires : m√©thodes directes

> üìö **Objectif du chapitre :** Utiliser les ordinateurs pour r√©soudre efficacement et surtout *de mani√®re stable* de larges syst√®mes d'√©quations lin√©aires sous forme matricielle $Ax = b$.

### 1. G√©n√©ralit√©s et le Conditionnement d'une Matrice

Un syst√®me lin√©aire peut s'√©crire sous forme vectorielle/matricielle compacte :

$$
Ax = b
$$

Avec $A$ une matrice $m \times n$, $x$ le vecteur des inconnues et $b$ le vecteur solution. Les m√©thodes directes de base (ce chapitre) se focalisent sur les syst√®mes **carr√©s (m = n) r√©guliers, c'est-√†-dire inversibles (d√©terminant non nul)**.

#### Le conditionnement du syst√®me lin√©aire $\kappa(A)$
De la m√™me mani√®re qu'une fonction au Chapitre 1 poss√®de un conditionnement, une **matrice poss√®de un conditionnement** face aux erreurs d'arrondis des donn√©es initiales.

**D√©monstration de la borne du conditionnement matriciel :**
Si on a une erreur pure num√©rique $\delta b$ sur le vecteur solution, alors le syst√®me r√©solu par la machine trouve $\hat{x} = x + \delta x$.
$$ A(x + \delta x) = b + \delta b \implies A \delta x = \delta b \implies \delta x = A^{-1} \delta b $$
En prenant les normes vectorielles de l'√©quation ci-dessus, et en utilisant les propri√©t√©s des normes ($\|M v\| \leq \|M\|\|v\|$) :
$$ \|\delta x\| \leq \|A^{-1}\| \|\delta b\| \quad \text{ (√âq. 1)} $$
Et on sait aussi gr√¢ce √† l'√©quation nominale $Ax = b$ que $\|b\| \leq \|A\| \|x\|$, ce qui implique que :
$$ \frac{1}{\|x\|} \leq \frac{\|A\|}{\|b\|} \quad \text{ (√âq. 2)} $$
Si on multiplie nos deux in√©quations (√âq. 1 et √âq. 2) membre √† membre pour former l'erreur relative c√¥t√© gauche :
$$
\frac{\|\delta x\|}{\|x\|} \leq \left( \|A^{-1}\| \|A\| \right) \frac{\|\delta b\|}{\|b\|}
$$

Cette formule relie le pire cas d'amplification d'erreur au produit de la norme de la matrice par la norme de son inverse. L'amplification fatale des impr√©cisions des donn√©es initiales d√©pendra donc **exclusivement des propri√©t√©s singuli√®res de la matrice $A$**.

On isole cette valeur sous le nom officiel de **Conditionnement d'une matrice** :
$$
\kappa(A) = \|A^{-1}\| \cdot \|A\|
$$

Si $\kappa(A) \approx 1$, le syst√®me est **bien conditionn√©**.
Si $\kappa(A) \gg 1$, le syst√®me est **mal conditionn√©** (exemple: la c√©l√®bre matrice de Hilbert).

### 2. Normes Vectorielles et Matricielles

Pour quantifier la "taille" des erreurs ou d'une matrice, on utilise des normes.

#### Normes Vectorielles (sur un vecteur $v$)
- **Norme 1 (Manhattan) :** $\|v\|_1 = \sum_{i=1}^n |v_i|$ *(Somme des valeurs absolues)*
- **Norme 2 (Euclidienne) :** $\|v\|_2 = \sqrt{\sum_{i=1}^n |v_i|^2}$ *(Distance g√©om√©trique ordinaire)*
- **Norme infinie (Max) :** $\|v\|_\infty = \max_{i=1 \dots n} |v_i|$ *(La plus grande composante gouverne)*

#### Normes Matricielles Induites
Une norme matricielle $\|A\|$ d√©coule des normes vectorielles, et repr√©sente "l'allongement maximum" que la matrice peut faire subir √† n'importe quel vecteur de longueur $1$ :

$$
\|A\| = \max_{v \neq 0} \frac{\|Av\|}{\|v\|}
$$

### 3. Les Mauvaises M√©thodes Directes
Avant d'√©tudier la Factorisation LU, tordons le cou √† deux tr√®s mauvaises id√©es informatiques :

1. **La m√©thode de Cramer ($x_i = \det(A_i) / \det(A)$)** : Pour un syst√®me √† $n$ inconnues, calculer ces d√©terminants co√ªte au minimum $\approx n!$ op√©rations (une complexit√© factorielle !). Un syst√®me $50 \times 50$ prendrait plus que l'√¢ge de l'univers.
2. **L'inversion matricielle brute ($x = A^{-1}b$) :** Calculer rigoureusement toute la matrice inverse $A^{-1}$ avant de multiplier par $b$ est environ $2.3 \times$ plus lent que l'algorithme "d'√©limination" qui m√®ne √† la solution de fa√ßon asym√©trique. (L'utilisation de la commande `inv(A)*b` est √† proscrire : on utilise la division gauche matricielle `A\b`).

### 4. R√©solution de r√©seaux triangulaires

R√©soudre un syst√®me dont la matrice est **uniquement construite par un triangle** est rapide, gratuit, et hyper stable.

Par exemple, un syst√®me triangulaire inf√©rieur $L$ ("Lower") :

$$
\begin{pmatrix} a_{11} & 0 & 0 \\ a_{21} & a_{22} & 0 \\ a_{31} & a_{32} & a_{33} \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} b_1 \\ b_2 \\ b_3 \end{pmatrix}
$$

Cette √©quation se r√©sout "en cascade √† l'endroit" (*forward-substitution*) ligne par ligne, avec une complexit√© d'√† peine $n^2$ op√©rations (flops), et sans aucune instabilit√© num√©rique, car on √©vite les grandes soustractions de masses. De m√™me, une matrice triangulaire sup√©rieure $U$ ("Upper") se r√©soudra par remont√©e (*backward-substitution*).

**L'id√©e de g√©nie de l'Analyse Num√©rique :** "Transformons tous les syst√®mes complexes et intriqu√©s (matrices pleines) en cascades de simples triangles."

### 5. L'√âlimination de Gauss et la Factorisation LU

**D√©monstration Algorithmique et Th√©or√®me :**
Prenons la matrice A :
$$
A = \begin{pmatrix} a_{11} & a_{12} & a_{1n} \\ a_{21} & a_{22} & a_{2n} \\ a_{n1} & a_{n2} & a_{nn} \end{pmatrix}
$$

**√âtape 1 :** On veut mettre des z√©ros dans la 1√®re colonne en dessous de $a_{11}$.
On multiplie A par une matrice d'√©limination $L_1$ :
$$
L_1 = \begin{pmatrix} 1 & 0 & 0 \\ -\frac{a_{21}}{a_{11}} & 1 & 0 \\ -\frac{a_{n1}}{a_{11}} & 0 & 1 \end{pmatrix}
\implies L_1 A = \begin{pmatrix} a_{11} & a_{12} & a_{1n} \\ 0 & a^{(2)}_{22} & a^{(2)}_{2n} \\ 0 & a^{(2)}_{n2} & a^{(2)}_{nn} \end{pmatrix}
$$
(O√π chaque fraction $-\frac{a_{i1}}{a_{11}}$ est le multiplicateur de ligne $k$).

**√âtape k :** On reproduit cela $(n-1)$ fois avec $L_2, L_3 \dots L_{n-1}$.
√Ä la fin, la partie inf√©rieure est remplie de z√©ros. Ce qu'il reste est une matrice purement triangulaire sup√©rieure qu'on appelle $U$.
$$
L_{n-1} \dots L_2 L_1 A = U
$$

**Le g√©nie du Th√©or√®me :** Si on rassemble tous ces modificateurs en les inversant du c√¥t√© du U, l'inverse de la matrice $(L_{n-1} \dots L_1)$ donne la prestigieuse matrice $L$.
Miraculeusement, $L$ est triangulaire inf√©rieure ET tous ses coefficients ne sont **rien d'autre que les coefficients multiplicateurs $l_{ik} = \frac{a_{ik}}{a_{kk}}$ bruts, rang√©s pile √† leur place !**
$$
L = (L_{n-1} \dots L_1)^{-1} = \begin{pmatrix} 1 & 0 & 0 \\ \frac{a_{21}}{a_{11}} & 1 & 0 \\ \frac{a_{n1}}{a_{11}} & \frac{a^{(2)}_{n2}}{a^{(2)}_{22}} & 1 \end{pmatrix}
$$

D'o√π l'incontournable **Th√©or√®me de la Factorisation LU** : Toute matrice r√©guli√®re peut √™tre scind√©e algorithmiquement en :
$$
\boxed{A = LU}
$$
O√π $L$ rassemble l'historique des multiplicateurs avec des $1$ sur la diagonale, et $U$ contient l'√©tat final des pivots.

Cette factorisation demande une puissance de calcul de :

$$
\text{Co√ªt de Factorisation LU} \approx \frac{2}{3}n^3 \text{ flops}
$$

*(C'est une complexit√© polynomiale $O(n^3)$ ce qui est massivement meilleur que $n!$).*

#### La R√©volution algorithmique de la s√©paration LU
Le v√©ritable int√©r√™t informatique d'imprimer la matrice "$L$" (au lieu d'effectuer le pivot de Gauss et d'oublier toutes les √©tapes) intervient si notre second membre $b$ change soudainement.

Pour r√©soudre n'importe qu'elle nouvelle √©quation $Ax = b'$, sachant qu'on a d√©j√† souffert pour extraire $A = LU$ (la d√©construction qui co√ªte le $\frac{2}{3}n^3$) :

$$
L \cdot U \cdot x = b'
$$

On s'y prend en deux temps. D'abord on pose la variable interne $(Ux) = y$ :
1. **√âtape de descente** : R√©soudre $Ly = b'$ (Cascade avant, co√ªte $n^2$ flops).
2. **√âtape de remont√©e** : R√©soudre $Ux = y$ (Cascade arri√®re, co√ªte $n^2$ flops).

Gr√¢ce √† cet enregistrement m√©moire, re-r√©soudre l'√©quation avec des variables externes alternatives devient ridiculeusement instantan√© (du $\sim O(n^2)$ !).

### 6. Le probl√®me de l'instabilit√© et la correction (Factorisation $PA=LU$)

> ‚ö†Ô∏è **Constat d'√©chec :** La factorisation na√Øve $A = LU$ n'est **pas stable**.

**D√©monstration du crash avec un petit pivot :**
Prenons la matrice :
$$ A = \begin{pmatrix} 10^{-20} & 1 \\ 1 & 1 \end{pmatrix} \quad (Ax=b \text{ avec } b=(1,2)^T) $$
Cette matrice a un $\kappa(A) \approx 2$ (Donc parfaite pour la machine a priori).
Cependant, l'algorithme $LU$ force la division par le pivot $(1, 1)$. Donc le multiplicateur $l_{21} = \frac{1}{10^{-20}} = 10^{20}$.
La matrice $U$ sera :
$$ U = \begin{pmatrix} 10^{-20} & 1 \\ 0 & 1 - 10^{20} \end{pmatrix} = \begin{pmatrix} 10^{-20} & 1 \\ 0 & -10^{20} \end{pmatrix} $$
 *(Car $1 \ominus 10^{20} = -10^{20}$ √† cause du nombre limit√© de bits de la mantisse, le "1" a √©t√© d√©truit et est tomb√© dans le n√©ant : Annulation catastrophique !)*
L'ordinateur vient de rater lamentablement le calcul en interne.

#### Le Pivotage Partiel
La parade : On choisit la ligne poss√©dant la **valeur absolue maximale** comme pivot en la permutant. 
Pour l'ordinateur, √©changer la ligne $1$ et $2$ correspond √† pr√©-multiplier l'√©quation $Ax=b$ par une matrice identit√© permut√©e $P$ :
$$
P(Ax) = P(b) \implies \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} \begin{pmatrix} 10^{-20} & 1 \\ 1 & 1 \end{pmatrix} x = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \end{pmatrix}
$$
La nouvelle matrice √† factoriser devient $\begin{pmatrix} 1 & 1 \\ 10^{-20} & 1 \end{pmatrix}$.
Maintenant, le pivot en $(1,1)$ est $1$. Le multiplicateur est $l_{21} = \frac{10^{-20}}{1} = 10^{-20}$. **Aucune erreur d'arrondi ou gonflement math√©matique infernal n'a lieu**. La stabilit√© est parfaite.

L'algorithme moderne d√©finitif est **Factorisation LU avec Pivotage ($PA = LU$)** :
$$
\boxed{PA = LU}
$$
O√π $P$ est une matrice de rep√©rage de lignes. **Le calcul $PA=LU$ est garanti inconditionnellement stable algorithmiquement.**

### 7. Applications annexes de la force LU

Le but de tout syst√®me matriciel n'est pas uniquement de trouver le vecteur $x$.
Les facteurs purs $L, U$ et $P$ ont d√©voil√© tous les secrets structurels profonds de la trame.

#### Calcul d'un D√©terminant lourd
Si on se rappelle qu'un d√©terminant matriciel se multiplie, alors :
$$ \det(P) \det(A) = \det(L) \det(U) $$
√âtant donn√© que $L$ et $U$ sont des matrices "triangulaires", leur d√©terminant s'√©l√®ve √† la simple... multiplication de toute leur propre diagonale ! Puisque la diagonale de $L$ ne contient que des $1$ (c'est forc√© par le design bas niveau de l'informatique), $\det(L) = 1$. Et le d√©terminant de $P$ est simplement $(-1)^p$ avec $p$ le nombre de lignes interchang√©es.

Le v√©ritable d√©terminant d'un monstre abstrait multidimensionnel $A$ est donc :
$$
\det(A) = (-1)^p \cdot u_{11} \cdot u_{22} \cdots u_{nn}
$$
*(En calculant la factorisation de co√ªt raisonnable, la vraie machine √† calculer Octave d√©livre les d√©terminants de mani√®re infiniment plus √©l√©gante et performante que la technique alg√©brique scolaire usuelle).*

#### Calcul profond de l'Inversion ($A^{-1}$)
L'inverse s'√©crit formellement en recherchant la collection des colonnes vectorielles $x_i$ r√©pondant √† l'ind√©pendance de la base canonnique : $A x_i = e_i$.
On ex√©cute l'algorithme de factorisation en cascade sur $PA=LU$ pour toutes les colonnes unitaires, co√ªtant au global un impressionnant total brut de : $\frac{8}{3}n^3$ flops en pur temps machine.
