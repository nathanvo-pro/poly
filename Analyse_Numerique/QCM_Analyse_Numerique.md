# ‚úÖ Quiz / QCM ‚Äî Analyse Num√©rique (MATH-F-2007)

> Quiz avec questions √† choix multiples pour r√©viser chaque chapitre.
> Cliquez sur **üí° Solution** pour v√©rifier votre r√©ponse et voir l'explication.

---

## Chapitre 1 ‚Äî Virgule flottante, IEEE 754 & Conditionnement

### Question 1.1 : En repr√©sentation IEEE 754 simple pr√©cision (32 bits), combien de bits sont allou√©s √† l'exposant ?
- [ ] A) 5 bits
- [ ] B) 8 bits
- [ ] C) 11 bits
- [ ] D) 23 bits

<details>
<summary>üí° Solution</summary>

**R√©ponse B**. En simple pr√©cision (32 bits), on a 1 bit de signe, 8 bits d'exposant (biais√© a 127) et 23 bits de mantisse.
</details>

### Question 1.2 : Qu'est-ce que l'erreur d'arrondi ou "machine epsilon" ($\epsilon_{mach}$) ?
- [ ] A) L'√©cart maximum entre deux nombres entiers cons√©cutifs
- [ ] B) Le plus petit nombre positif repr√©sentable par la machine
- [ ] C) La borne sup√©rieure de l'erreur relative lors de l'arrondi d'un nombre r√©el √† son nombre flottant le plus proche
- [ ] D) L'erreur g√©n√©r√©e lorsqu'on divise par z√©ro

<details>
<summary>üí° Solution</summary>

**R√©ponse C**. Le $\epsilon_{mach}$ limite l'erreur relative maximale d'arrondi : $\frac{|fl(x) - x|}{|x|} \le \epsilon_{mach}$.
</details>

### Question 1.3 : Un probl√®me math√©matique est dit "mal conditionn√©" si :
- [ ] A) L'algorithme utilis√© pour le r√©soudre effectue trop d'op√©rations.
- [ ] B) De petites perturbations dans les donn√©es d'entr√©e provoquent de petites variations dans la solution.
- [ ] C) De petites perturbations dans les donn√©es d'entr√©e provoquent de grandes variations dans la solution.
- [ ] D) La matrice associ√©e est de taille impaire.

<details>
<summary>üí° Solution</summary>

**R√©ponse C**. Le conditionnement est une propri√©t√© intrins√®que du probl√®me. S'il est grand, une infime erreur sur les donn√©es (comme l'arrondi) est amplifi√©e dans le r√©sultat.
</details>

### Question 1.4 : Le ph√©nom√®ne d'¬´ annulation catastrophique ¬ª se produit particuli√®rement dans le cas :
- [ ] A) De la multiplication de deux tr√®s grands nombres.
- [ ] B) D'une division par un nombre proche de z√©ro.
- [ ] C) De la soustraction de deux nombres flottants tr√®s proches l'un de l'autre.
- [ ] D) Du calcul du logarithme de 1.

<details>
<summary>üí° Solution</summary>

**R√©ponse C**. Soustraire deux quantit√©s presque √©gales fait "perdre" les chiffres significatifs de t√™te et donne un r√©sultat bas√© presque enti√®rement sur l'erreur de repr√©sentation, ce qui ruine la pr√©cision.
</details>

### Question 1.5 : Laquelle de ces expressions √©quivalentes math√©matiquement est num√©riquement la plus stable pour √©valuer $\sqrt{x^2 + 1} - 1$ quand $x \approx 0$ ?
- [ ] A) $\sqrt{x^2 + 1} - 1$
- [ ] B) $\frac{x^2}{\sqrt{x^2 + 1} + 1}$
- [ ] C) $\frac{1}{\sqrt{x^2 + 1} - 1}$
- [ ] D) $x^2 + 1$

<details>
<summary>üí° Solution</summary>

**R√©ponse B**. L'expression A subit une annulation (car $\sqrt{1} - 1 = 0$). L'expression B, obtenue en multipliant par le conjugu√©, √©vite cette soustraction critique de "presque 1" par "1" et reste donc stable.
</details>

---

## Chapitre 2 ‚Äî Syst√®mes Lin√©aires & D√©composition LU / Cholesky

### Question 2.1 : Quel est le co√ªt algorithmique principal (la complexit√©) de la m√©thode d'√©limination de Gauss pour une matrice pleine $n \times n$ ?
- [ ] A) $O(n^2)$ op√©rations
- [ ] B) $\approx \frac{2n^3}{3}$ op√©rations
- [ ] C) $O(n \log n)$ op√©rations
- [ ] D) $\approx \frac{n^4}{4}$ op√©rations

<details>
<summary>üí° Solution</summary>

**R√©ponse B**. C'est un processus en $O(n^3)$. Plus pr√©cis√©ment on compte environ $\frac{2n^3}{3}$ op√©rations flottantes.
</details>

### Question 2.2 : Dans la d√©composition LU de Crout ou Doolittle ($A = LU$), quelles propri√©t√©s ont L et U ?
- [ ] A) L est triangulaire inf√©rieure, U est orthogonale.
- [ ] B) L est triangulaire inf√©rieure, U est triangulaire sup√©rieure.
- [ ] C) L et U sont toutes deux sym√©triques.
- [ ] D) L est une matrice de permutation, U est triangulaire.

<details>
<summary>üí° Solution</summary>

**R√©ponse B**. Lower (Inf√©rieure) et Upper (Sup√©rieure). Cette d√©composition permet de r√©soudre $Ax=b$ en deux substitutions $Ly=b$ et $Ux=y$.
</details>

### Question 2.3 : Quel est le r√¥le principal de la "strat√©gie de pivot partiel" dans la r√©solution de syst√®mes lin√©aires ?
- [ ] A) R√©duire la complexit√© temporelle de $O(n^3)$ √† $O(n^2)$.
- [ ] B) Permettre d'inverser les matrices singuli√®res.
- [ ] C) Accro√Ætre la stabilit√© num√©rique en √©vitant la division par des pivots trop petits (proches de 0).
- [ ] D) Rendre la matrice sym√©trique d√©finie positive.

<details>
<summary>üí° Solution</summary>

**R√©ponse C**. √âchanger les lignes pour placer le plus grand √©l√©ment possible en position de pivot minimise l'amplification des erreurs d'arrondi (instabilit√©).
</details>

### Question 2.4 : √Ä quelle condition stricte peut-on toujours appliquer la d√©composition de **Cholesky** ($A = LL^T$) pour une matrice r√©elle $A$ ?
- [ ] A) $A$ doit √™tre une matrice Diagonale Dominante.
- [ ] B) $A$ doit √™tre tridiagonale.
- [ ] C) $A$ doit √™tre Sym√©trique et D√©finie Positive (SDP).
- [ ] D) $A$ doit √™tre de conditionnement nul.

<details>
<summary>üí° Solution</summary>

**R√©ponse C**. Cholesky est deux fois plus rapide (et plus stable naturellement sans pivotage) mais elle exige math√©matiquement que $A = A^T$ et que $x^T A x > 0$ pour tout $x \neq 0$.
</details>

### Question 2.5 : L'indice de conditionnement d'une matrice inversible $A$, not√© $\kappa(A)$ en norme quelconque, est calcul√© comme suit :
- [ ] A) $\kappa(A) = ||A|| + ||A^{-1}||$
- [ ] B) $\kappa(A) = \det(A) \cdot \det(A^{-1})$
- [ ] C) $\kappa(A) = \frac{\lambda_{max}(A)}{\lambda_{min}(A)}$ uniquement
- [ ] D) $\kappa(A) = ||A|| \cdot ||A^{-1}||$

<details>
<summary>üí° Solution</summary>

**R√©ponse D**. Le conditionnement s'√©crit formellement par le produit des normes. S'il est tr√®s grand $\kappa(A) \gg 1$, le syst√®me $Ax=b$ est mal conditionn√© et sa solution num√©rique est instable.
</details>

---

## Chapitre 3 ‚Äî √âquations Non-Lin√©aires & Recherche de Racines

### Question 3.1 : Quelles sont les exigences sur la fonction $f$ pour appliquer le th√©or√®me de Bolzano (base de la m√©thode de bissection) sur l'intervalle $[a, b]$ ?
- [ ] A) $f$ doit √™tre continue et $f(a) \cdot f(b) < 0$.
- [ ] B) $f$ doit √™tre strictement monotone convexe.
- [ ] C) $f$ doit √™tre d√©rivable partout.
- [ ] D) L'intervalle doit contenir l'origine z√©ro.

<details>
<summary>üí° Solution</summary>

**R√©ponse A**. Si la fonction est continue et change de signe aux bornes ($f(a) \cdot f(b) < 0$), le th√©or√®me des valeurs interm√©diaires (ou Bolzano) garantit qu'il existe au moins une racine dans $[a, b]$.
</details>

### Question 3.2 : Quelle est la vitesse de convergence (l'ordre) de la m√©thode de Newton-Raphson lorsque l'on est proche d'une racine **simple** ?
- [ ] A) Convergence Lin√©aire (Ordre 1)
- [ ] B) Convergence Super-Lin√©aire (Ordre 1.618)
- [ ] C) Convergence Quadratique (Ordre 2)
- [ ] D) Convergence Cubique (Ordre 3)

<details>
<summary>üí° Solution</summary>

**R√©ponse C**. La grande force de Newton est sa convergence quadratique. L'erreur √† l'√©tape $n+1$ est approximativement proportionnelle au carr√© de l'erreur √† l'√©tape $n$.
</details>

### Question 3.3 : Quelle est la formule it√©rative canonique de la m√©thode de Newton-Raphson pour trouver la racine de $f(x)=0$ ?
- [ ] A) $x_{k+1} = x_k - \frac{f(x_k)}{2}$
- [ ] B) $x_{k+1} = \frac{x_k + f(x_k)}{f'(x_k)}$
- [ ] C) $x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}$
- [ ] D) $x_{k+1} = x_k \cdot f(x_k) - f'(x_k)$

<details>
<summary>üí° Solution</summary>

**R√©ponse C**. Formule de Newton. Elle se base g√©om√©triquement sur l'intersection de la tangente √† la courbe de $f$ en $x_k$ avec l'axe des abscisses.
</details>

### Question 3.4 : Que se passe-t-il pour la m√©thode de Newton si la racine $\alpha$ recherch√©e est *multiple* (par ex. racine double, $f(\alpha)=0$ et $f'(\alpha)=0$) ?
- [ ] A) La m√©thode n'est pas du tout capable d'√©valuer la fonction.
- [ ] B) La m√©thode de Newton conserve son ordre de convergence quadratique sans probl√®me.
- [ ] C) La m√©thode de Newton perd sa convergence quadratique et devient seulement lin√©aire.
- [ ] D) Newton part instantan√©ment vers l'infini.

<details>
<summary>üí° Solution</summary>

**R√©ponse C**. La racine de la d√©riv√©e modifie le profil d'erreur. Si la multiplicit√© est $>1$, Newton ne converge que de fa√ßon lin√©aire. On peut "modifier" Newton en $x - \frac{m f(x)}{f'(x)}$ pour retrouver un taux quadratique.
</details>

### Question 3.5 : En quoi la m√©thode de la *S√©cante* diff√®re-t-elle principalement de la m√©thode de *Newton* ?
- [ ] A) Elle n√©cessite la d√©riv√©e seconde $f''(x)$ pour chaque it√©ration.
- [ ] B) Elle utilise l'intervalle entier, donc elle exige un changement de signe strict $f(a) \cdot f(b) < 0$.
- [ ] C) Elle approxime la d√©riv√©e par le quotient diff√©rentiel entre les deux derniers points calcul√©s, supprimant le besoin de calculer analytiquement $f'(x)$.
- [ ] D) Elle converge toujours plus vite que Newton.

<details>
<summary>üí° Solution</summary>

**R√©ponse C**. La S√©cante est id√©ale quand la d√©riv√©e $f'(x)$ est trop difficile ou co√ªteuse √† obtenir. Son ordre de convergence est de $1.618$ (nombre d'or), soit un peu moins performant que Newton mais tr√®s pratique.
</details>

