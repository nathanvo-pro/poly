# Exercices ‚Äî Analyse Num√©rique

## Chapitre 1 : Repr√©sentation en virgule flottante, stabilit√© et conditionnement

> üìà **Difficult√© croissante :** Les exercices vont des d√©finitions de base (‚≠ê) aux probl√®mes de r√©flexion math√©matique (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê). Pensez √† chercher sur papier avant d'ouvrir les solutions !

---

### ‚≠ê Niveau 1 ‚Äî Fondamentaux de la virgule flottante

---

**Exercice 1 ‚Äî Vocabulaire**

Dans l'expression standard $\pm 0.d_1 d_2 \cdots d_t \cdot \beta^e$, √† quoi correspondent exactement les lettres $t$, $\beta$, $e$ et $d_i$ ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

- $\beta$ : La base du syst√®me (binaire $\beta=2$ ou d√©cimale $\beta=10$)
- $t$ : Le nombre exact de chiffres significatifs (qui dicte la pr√©cision)
- $e$ : L'exposant (qui dicte l'ordre de grandeur, encadr√© par $e_{\min}$ et $e_{\max}$)
- $d_i$ : Le $i$-√®me chiffre (encadr√© par $0 \le d_i \le \beta - 1$)

L'ensemble des chiffres forme ce qu'on appelle la **mantisse**.
</details>

---

**Exercice 2 ‚Äî Repr√©sentation Normalis√©e**

Qu'est-ce qu'une mantisse normalis√©e ? Pourquoi est-ce un avantage massif pour les syst√®mes binaires ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

Une repr√©sentation est **normalis√©e** si son tout premier chiffre significatif est non nul ($d_1 \neq 0$). 
Cela permet d'avoir une repr√©sentation unique pour chaque nombre (par exemple, on interdit l'ambigu√Øt√© entre $2 = 0.2 \cdot 10^1 = 0.02 \cdot 10^2$).

**L'avantage massif en binaire :** Si la base est $2$, les seuls chiffres possibles sont $0$ et $1$. Puisque $d_1$ ne peut pas √™tre $0$, **il est obligatoirement √©gal √† $1$**. Puisqu'on le sait toujours √† l'avance, on n'a plus besoin de le stocker en m√©moire ! Cela fait gagner $1$ bit de pr√©cision gratuitement (le bit implicite).
</details>

---

**Exercice 3 ‚Äî D√©monstration de l'Unit√© d'arrondi $u$**

√ânoncez la formule de l'unit√© d'arrondi $u$ relative √† la virgule flottante. Ensuite, **d√©montrez math√©matiquement** cette borne sup√©rieure √† partir de la distance absolue maximale induite par l'arrondi du $(t+1)$-√®me chiffre.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

L'unit√© d'arrondi r√©pond √† la formule :
$$
u = \frac{1}{2}\beta^{1-t}
$$

**D√©monstration formelle :**
Soit un r√©el strict $x = \pm 0.d_1 d_2 \cdots d_t d_{t+1} \cdots \cdot \beta^e$.
Son √©quivalent machine $\text{fl}(x)$ ne garde que $t$ chiffres. L'arrondi ("au plus proche") d√©pend du chiffre $d_{t+1}$. L'√©cart absolu maximal entre la vraie valeur et le flottant machine ne d√©passera jamais la moiti√© du poids du dernier chiffre (le $t$-i√®me chiffre) :
$$
|x - \text{fl}(x)| \leq \frac{1}{2} \beta^{-t} \cdot \beta^e
$$
Ensuite, on cherche l'erreur relative maximale, on divise donc tout par $|x|$.
La plus petite valeur que peut prendre la mantisse de $|x|$ (condition critique) est $1.00\dots \times \beta^{-1}$ (puisque $d_1 \geq 1$).
Donc $|x|_{\min} = \beta^{-1} \cdot \beta^e = \beta^{e-1}$.
Divisons :
$$
\text{Erreur relative max} = \frac{|x - \text{fl}(x)|}{|x|} \leq \frac{\frac{1}{2} \beta^{-t} \beta^e}{\beta^{e-1}} = \frac{1}{2}\beta^{1-t}
$$
Cette borne universelle d'erreur s'appelle $u$.
</details>

---

### ‚≠ê‚≠ê Niveau 2 ‚Äî Le Standard IEEE et l'Erreur Relative

---

**Exercice 4 ‚Äî Erreur absolue vs Erreur relative**

Si je souhaite stocker la valeur absolue $A = 1000$ et qu'elle devient $\hat{A} = 1000.5$, calculez l'erreur absolue puis l'erreur relative.
Recommencez si je veux stocker la valeur $B = 0.001$ et qu'elle devient $\hat{B} = 0.0015$. Quelle conclusion en tirer ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

**Pour A ($1000 \to 1000.5$) :**
- Erreur absolue = $|1000.5 - 1000| = 0.5$
- Erreur relative = $0.5 / 1000 = 0.0005$ soit $\approx 0.05\%$

**Pour B ($0.001 \to 0.0015$) :**
- Erreur absolue = $|0.0015 - 0.001| = 0.0005$
- Erreur relative = $0.0005 / 0.001 = 0.5$ soit $\approx 50\%$

**Conclusion :** 
Ici l'erreur absolue sur $B$ est de seulement $0.0005$ (ce qui semble insignifiant), mais proportionnellement √† sa petite grandeur, on se trompe de $50\%$ ! L'erreur relative est la seule m√©trique fiable en analyse num√©rique pour √©valuer l'impact r√©el d'une impr√©cision.
</details>

---

**Exercice 5 ‚Äî Les limites (Overflow, Underflow)**

Que devient l'√©tat du processeur si l'on calcule :
1. $2 \cdot x_{\max}$
2. $x_{\min} / 2$
3. $0.0 / 0.0$

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

1. **Overflow (d√©passement sup√©rieur) :** La valeur d√©passe le maximum stockable pour l'exposant. Le standard dicte de renvoyer la valeur sp√©ciale `Inf` (Infini).
2. **Underflow (d√©passement inf√©rieur) :** La valeur devient plus petite que nos crans d'√©chelle. Cependant, plut√¥t que de crasher vers $0$, on passe √† une repr√©sentation **d√©normalis√©e**, mais en sacrifiant la pr√©cision (l'unit√© $u$ n'est plus garantie).
3. **NaN (Not a Number) :** Forme ind√©termin√©e qui ne donne aucun sens math√©matique.
</details>

---

### ‚≠ê‚≠ê‚≠ê Niveau 3 ‚Äî Propagation et Annulation

---

**Exercice 6 ‚Äî Le Mod√®le d'Arithm√©tique Standard**

Dites comment la machine calcule math√©matiquement l'op√©ration abstraite "$\circledcirc$" (qui remplace l'op√©ration math√©matique exacte $\circ$) entre deux flottants $A$ et $B$.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

Selon la sp√©cification IEEE, le processeur ex√©cute l'op√©ration exacte puis s'autorise une impr√©cision d'arrondi sur ce r√©sultat :

$$
A \circledcirc B = (A \circ B)(1 + \varepsilon), \quad |\varepsilon| \leq u
$$

Cette formule universelle permet de pr√©dire de combien s'amplifiera l'erreur relative apr√®s des centaines de multiplications/soustractions.
</details>

---

**Exercice 7 ‚Äî D√©monstration formelle de l'Annulation Catastrophique ($\kappa$)**

Montrez un exemple chiffr√© basique o√π la soustraction de nombres an√©antit la pr√©cision $u$.
Ensuite, utilisez la formule du Conditionnement d'un sous-probl√®me diff√©rentiable $\kappa(x)$ pour **prouver math√©matiquement** que la soustraction de deux nombres extr√™mement proches provoque une explosion exponentielle des erreurs initiales vers l'infini.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

**1. L'Exemple chiffr√© (Le massacre de la mantisse) :**
Imaginons un syst√®me qui ne garde que $6$ chiffres significatifs exacts (comme la simple pr√©cision $\approx$ 7 chiffres).
Soit $x = 1.000000$ et $y = 0.999999$. Les deux sont tr√®s pr√©cis.
Soustrayons :
$$
x - y = 0.000001
$$
Le r√©sultat $\hat{y}$ est normalis√© par la machine (puisqu'une mantisse ne peut pas commencer par des z√©ros). Elle le d√©cale comme $1.00000 \cdot 10^{-6}$. Cependant, √† cause de l'exposant n√©gatif, la machine **comble le vide de derri√®re avec du bruit num√©rique al√©atoire latent** (car la soustraction a d√©truit les seuls chiffres rigoureusement connus). 

**2. La Preuve math√©matique par le conditionnement :**
Posons la fonction $f(x) = x_1 - x_2$.
Son vecteur de d√©riv√©es partielles est $f'(x) = [1, -1]$. On prend la norme euclidienne $\|f'(x)\|_2 = \sqrt{1^2 + (-1)^2} = \sqrt{2}$.

Ins√©rons dans la formule jacobienne de $\kappa$ locale :
$$
\kappa(\text{soustraction}) = \frac{\|f'(x)\| \cdot \|x\|}{|f(x)|} = \frac{\sqrt{2} \cdot \sqrt{x_1^2 + x_2^2}}{|x_1 - x_2|}
$$

**Conclusion finale :** Si nos valeurs sont proches ($x_1 \approx x_2$), le d√©nominateur de la fraction tend furieusement vers $0$, tandis que le num√©rateur reste hautement positif (les carr√©s).
Cons√©quence indiscutable de l'alg√®bre : $\kappa \to \infty$. 
Le conditionnement du probl√®me devient dramatique, la moindre impr√©cision machine d'entr√©e explosera d'un facteur immense dans la sortie finale de la soustraction.
</details>

---

### ‚≠ê‚≠ê‚≠ê‚≠ê Niveau 4 ‚Äî Stabilit√© et Conditionnement

---

**Exercice 8 ‚Äî Stabilit√© algorihtmique (Directe et Inverse)**

D√©crivez avec des mots simples (et la formule de base avec $C$) la diff√©rence entre un algorithme qui b√©n√©ficie de la *stabilit√© directe* par rapport √† la *stabilit√© inverse*. Lequel implique directement l'autre ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

- **Stabilit√© Directe :** Mon algorithme se trompe l√©g√®rement ($\hat{y} - y$ est petit). Mais rassurons-nous : sa marge d'erreur directe n'est *pas pire* que si j'avais donn√© des donn√©es minusculement perturb√©es √† une fonction parfaite.
- **Stabilit√© Inverse :** C'est un concept encore plus solide. Il dit : Mon r√©sultat inexact et imparfait ($\hat{y}$) N'EST PAS juste "proche", il **EST** LA r√©ponse parfaite et math√©matique √† un probl√®me ($\Delta x$) qui fr√¥le le mien de tr√®s pr√®s ($\leq u$).

$$
\text{Stabilit√© Inverse} : \quad f(x + \Delta x) = \hat{y} \quad \text{avec } \frac{\|\Delta x\|}{\|x\|} \leq C \cdot u
$$

**Conclusion :** La stabilit√© inverse est beaucoup plus robuste. C'est elle qui **implique et entra√Æne automatiquement** la stabilit√© directe.
</details>

---

**Exercice 9 ‚Äî Calcul du Conditionnement $\kappa(x)$**

Quelle est la formule officielle pour mesurer le conditionnement d'un probl√®me avec une fonction diff√©rentiable ? Prouvez que le calcul de la racine carr√©e $f(x) = \sqrt{x}$ est extr√™mement bien conditionn√© num√©riquement, sans lien avec l'algorithme choisi.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

La formule utilisant la Jacobienne / d√©riv√©e est :

$$
\kappa(x) = \frac{\|f'(x)\| \cdot \|x\|}{\|f(x)\|}
$$

Appliqu√© √† notre racine carr√©e $f(x) = x^{1/2}$ et donc $f'(x) = \frac{1}{2\sqrt{x}}$ :

$$
\kappa(x) = \frac{\frac{1}{2\sqrt{x}} \cdot x}{\sqrt{x}} = \frac{\frac{x}{2\sqrt{x}}}{\sqrt{x}} = \frac{1}{2}
$$

Ici $\kappa(x) = 1/2$. √âtant donn√© que le conditionnement est aux alentours de $1$ (et encore mieux : il pond√®re/r√©duit l'erreur), ce probl√®me physique tol√®re excessivement bien l'impr√©cision et ne causera aucun comportement chaotique ! C'est remarquablement bien conditionn√©.
</details>

---

### ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Niveau 5 ‚Äî Probl√®mes Num√©riques Complexes

---

**Exercice 10 ‚Äî Le Pi√®ge de la Soustraction Analys√© via $\kappa(x)$**

En utilisant la formule du conditionnement $\kappa(x)$ ci-dessus, prouvez math√©matiquement pourquoi la fonction de soustraction $f(x_1, x_2) = x_1 - x_2$ devient dramatiquement instable (Conditionnement vers l'infini) d√®s que $x_1$ et $x_2$ se rapprochent tr√®s fort. 

(Utilisez la norme vectorielle euclidienne $\|x\| = \sqrt{x_1^2 + x_2^2}$).

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

Notre fonction √† deux variables est $f(x_1, x_2) = x_1 - x_2$.
Le gradient (la d√©riv√©e) par rapport aux composantes donne le vecteur $f'(x) = (1, -1)$.
La norme de ce gradient est $\|(1, -1)\| = \sqrt{1^2 + (-1)^2} = \sqrt{2}$.

Appliquons la formule du conditionnement pour multivariables :

$$
\kappa(x_1, x_2) = \frac{\|f'(x)\| \cdot \|(x_1, x_2)\|}{\|f(x_1, x_2)\|}
$$

En ins√©rant la norme du gradient ($\sqrt{2}$), la norme des entr√©es ($\sqrt{x_1^2 + x_2^2}$) et la valeur absolue formelle de la fonction au d√©nominateur :

$$
\kappa(x_1, x_2) = \frac{\sqrt{2} \cdot \sqrt{x_1^2 + x_2^2}}{|x_1 - x_2|}
$$

**Que se passe-t-il lorsque $x_1 \approx x_2$ ?**
Le num√©rateur reste strictement positif (car les carr√©s s'additionnent et valent en gros $2 \cdot x_1^2$).
Mais le d√©nominateur $|x_1 - x_2|$ **tend farouchement vers z√©ro** !
Un nombre positif divis√© par presque-z√©ro explose vers l'infini. 
Donc : **$\kappa(x_1 \approx x_2) \to \infty$**. 

Le petit $\Delta x$ d'incertitude dans l'ordinateur se verra multipli√© par un million ou un milliard en franchissant cette op√©ration. Le probl√®me est horriblement **mal conditionn√©** !
</details>

---

## Chapitre 2 : Syst√®mes d'√©quations lin√©aires : m√©thodes directes

> üìà **Difficult√© croissante :** De la compr√©hension des normes (‚≠ê) √† la manipulation matricielle experte sur la factorisation PA=LU (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê). Pensez √† l'alg√®bre lin√©aire !

---

### ‚≠ê Niveau 1 ‚Äî Fondamentaux du Conditionnement $\kappa(A)$

---

**Exercice 11 ‚Äî La formule de $\kappa(A)$**

Donnez la d√©finition math√©matique du conditionnement d'une matrice $A$ r√©guli√®re, not√© $\kappa(A)$. Que signifie physiquement un conditionnement de $1$ ? Et un conditionnement de $10^{15}$ ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

La d√©finition officielle du conditionnement matriciel est :

$$
\kappa(A) = \|A^{-1}\| \cdot \|A\|
$$

- Si $\kappa(A) \approx 1$ : Le syst√®me est excellemment bien conditionn√©. Une petite erreur dans les donn√©es (le vecteur $b$) entra√Ænera une erreur de la m√™me taille dans le r√©sultat (le vecteur $x$).
- Si $\kappa(A) \approx 10^{15}$ : Le syst√®me est catastrophiquement mal conditionn√©. La moindre d√©cimale fausse dans l'√©quation de base sera amplifi√©e $10^{15}$ fois dans le r√©sultat final ! C'est g√©n√©ralement le cas de matrices presque singuli√®res.
</details>

---

**Exercice 12 ‚Äî Preuve du th√©or√®me d'amplification d'erreur**

Supposons le syst√®me exact $Ax = b$. √Ä cause de l√©g√®res d√©rives mat√©rielles, l'ordinateur r√©sout en r√©alit√© le syst√®me perturb√© $(A+\delta A)(x+\delta x) = (b + \delta b)$. Pour simplifier, assumons qu'il n'y a un bruit d'incertitude que sur le second membre analytique $b$ (donc $\delta A = 0$).

**D√©montrez math√©matiquement** (en utilisant les propri√©t√©s de l'inverse $A^{-1}$ et des normes $\leq$) le th√©or√®me d'amplification d'erreur qui prouve que l'erreur relative de la solution calcul√©e d√©pend d'un multiplicateur purement matriciel. Identifier le nom de ce modificateur.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

**Preuve √©tape par √©tape :**
Soit l'√©quation du syst√®me perturb√© uniquement en $b$ :
$$ A(x + \delta x) = b + \delta b $$
En distribuant, on a $Ax + A\delta x = b + \delta b$.
Or, on sait fondamentalement que la r√©ponse id√©ale est $Ax = b$. Ces termes purs s'annulent de chaque c√¥t√© :
$$ A \delta x = \delta b $$

L'ordinateur trouve son erreur r√©elle $\delta x$ (le parasite inject√©) en inversant la d√©pendance :
$$ \delta x = A^{-1} \delta b $$

On applique ensuite les normes. La propri√©t√© des normes induites donne $\|M v\| \leq \|M\| \|v\|$ :
$$ \|\delta x\| \leq \|A^{-1}\| \|\delta b\| \quad \text{(√âq. 1)} $$

Pour exprimer l'amplification *relative*, on doit diviser par la norme pure de la vraie solution $x$.
On sait que $Ax = b \implies \|b\| \leq \|A\| \|x\|$.
Inversons magiquement de bord cette in√©quation rigoureuse :
$$ \frac{1}{\|x\|} \leq \frac{\|A\|}{\|b\|} \quad \text{(√âq. 2)} $$

Multiplions intelligemment la partie gauche de l'√âq.1 par la partie gauche de l'√âq.2 :
$$
\frac{\|\delta x\|}{\|x\|} \leq \|A^{-1}\| \|\delta b\| \cdot \frac{\|A\|}{\|b\|}
$$

On r√©ordonne l'in√©quation en isolant le bloc pur des erreurs quantifi√©es :
$$
\frac{\|\delta x\|}{\|x\|} \le \bigl( \|A^{-1}\| \cdot \|A\| \bigr) \frac{\|\delta b\|}{\|b\|}
$$

**Conclusion :** 
L'amplification de l'erreur ne d√©pend **absolument que de la matrice de base**, peu importe l'algorithme informatique utilis√©.
Ce rapport s'appelle le Conditionnement de la Matrice $\kappa(A) = \|A^{-1}\| \cdot \|A\|$.
</details>

---

### ‚≠ê‚≠ê Niveau 2 ‚Äî Algorithmique des Matrices

---

**Exercice 13 ‚Äî Normes Vectorielles ($L_1, L_2, L_\infty$)**

Soit le vecteur $v = \begin{pmatrix} -3 \\ 1 \\ 4 \end{pmatrix}$. 
Calculez manuellement ses trois normes principales : $\|v\|_1$, $\|v\|_2$, et $\|v\|_\infty$.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

- **Norme 1 (Somme des valeurs absolues) :** 
  $\|v\|_1 = |-3| + |1| + |4| = 3 + 1 + 4 = 8$
- **Norme 2 (Euclidienne, racine de la somme des carr√©s) :** 
  $\|v\|_2 = \sqrt{(-3)^2 + 1^2 + 4^2} = \sqrt{9 + 1 + 16} = \sqrt{26} \approx 5.1$
- **Norme infini (Le max absolu) :** 
  $\|v\|_\infty = \max(|-3|, |1|, |4|) = 4$
</details>

---

**Exercice 14 ‚Äî Les mauvaises pratiques ($n!$ vs $O(n^3)$)**

Pourquoi, pour r√©soudre informatiquement $Ax = b$, est-il rigoureusement interdit d'utiliser la **m√©thode de Cramer** (calcul des d√©terminants successifs) ou de **calculer formellement l'inverse $A^{-1}$** ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

- La m√©thode de Cramer exige un grand nombre de d√©terminants lourds. R√©soudre un syst√®me de taille $n$ prendrait $\approx n!$ op√©rations. (*Ex : Une matrice $100 \times 100$ prendrait des milliards d'ann√©es √† l'ordinateur le plus puissant du monde !*).
- Calculer explicitement l'inverse $A^{-1}$ avant de multiplier par $b$ implique de faire $3$ fois trop de calculs inutiles pour rien. L'inversion matricielle prend au minimum $\approx \frac{8}{3}n^3$ flops, alors qu'une m√©thode directe et asym√©trique asym√©trique (comme LU) ne prend que $\approx \frac{2}{3}n^3$ flops. L'inverse prend **$2.3 \times$ plus de temps** √† un processeur moderne sans aucun gain en pr√©cision.
</details>

---

### ‚≠ê‚≠ê‚≠ê Niveau 3 ‚Äî M√©thode de Gauss et Syst√®mes Triangulaires

---

**Exercice 15 ‚Äî La b√©n√©diction des Matrices Triangulaires**

Pourquoi l'analyse num√©rique r√©duit-elle syst√©matiquement ses grilles √† des **matrices triangulaires** (Inf√©rieures $L$ ou Sup√©rieures $U$) ? Quelle est la complexit√© algorithmique pour r√©soudre $Ux = y$ ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

Un syst√®me triangulaire comme $Ux = y$ a la d√©licieuse propri√©t√© d'avoir d√©j√† sa derni√®re ligne compl√®tement r√©solue ! (ex: $a_{nn}x_n = y_n \implies x_n = y_n/a_{nn}$).
Il suffit d'isoler $x_n$ puis d'injecter la r√©ponse dans la ligne au dessus, en remontant (C'est la m√©thode de *Substitution arri√®re* / *Backward substitution*).

Ce processus est :
1. Extr√™mement rapide : Seulement $n^2$ flops (Op√©rations polynomiales de degr√© $2$, donc trivial pour un ordinateur).
2. Toujours math√©matiquement rigoureux et stable, car aucune annulation catastrophique massive n'a lieu.
</details>

---

**Exercice 16 ‚Äî Transformation de Gauss en Matrice**

L'√©limination classique de Gauss consiste √† soustraire multiple d'une ligne d'une autre (ex: $L_2 = L_2 - (\frac{a_{21}}{a_{11}})L_1$). 
En Analyse Num√©rique, comment mod√©lise-t-on cela sous le spectre de l'Alg√®bre ? Que devient le syst√®me $A$ √† la fin de la proc√©dure ? 

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

On mod√©lise chaque annulation de Gauss non pas comme une manipulation externe, mais intrins√®quement comme la multiplication de $A$ par une matrice √©l√©mentaire Inf√©rieure $L_1, L_2, \dots$

Apr√®s avoir inject√© plusieurs matrices $L_i$ (qui √©crasent la moiti√© bas-gauche pour y mettre des $0$), la matrice compl√®te d'origine **fusionne en un gigantesque triangle sup√©rieur**. On l'appelle $U$.

$$
(L_{n-1} \dots L_2 \cdot L_1) \cdot A = U
$$
</details>

---

### ‚≠ê‚≠ê‚≠ê‚≠ê Niveau 4 ‚Äî La Factorisation LU

---

**Exercice 17 ‚Äî Ex√©cution algorithmique de la Factorisation $A = LU$**

D√©montrez la puissance de Gauss algorithmique. D√©composez manuellement la matrice $A$ ci-dessous en ses facteurs $L$ et $U$ en d√©taillant imp√©rativement les √©tapes interm√©diaires (les duplicateurs $l_{ik}$).
$$
A = \begin{pmatrix} 2 & 1 & 1 \\ 4 & 1 & 0 \\ -2 & 2 & 1 \end{pmatrix}
$$

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

**√âtape 1 : Z√©ros sous le premier pivot ($a_{11} = 2$)**
Multiplicateur ligne 2 : $l_{21} = \frac{4}{2} = 2$
Multiplicateur ligne 3 : $l_{31} = \frac{-2}{2} = -1$

On applique $L_2 = L_2 - 2 L_1$ et $L_3 = L_3 - (-1) L_1$ :
$$
A^{(2)} = \begin{pmatrix} 2 & 1 & 1 \\ 0 & -1 & -2 \\ 0 & 3 & 2 \end{pmatrix}
$$
*Note : On range d√©j√† nos multiplicateurs ($2$ et $-1$) dans la 1√®re colonne de notre future matrice $L$ !*

**√âtape 2 : Z√©ro sous le deuxi√®me pivot ($a^{(2)}_{22} = -1$)**
Multiplicateur ligne 3 : $l_{32} = \frac{3}{-1} = -3$

On applique $L_3 = L_3 - (-3) L_2$ :
$$
A^{(3)} = \begin{pmatrix} 2 & 1 & 1 \\ 0 & -1 & -2 \\ 0 & 0 & -4 \end{pmatrix}
$$

**Finalisation Th√©orique :**
La matrice r√©sultante $A^{(3)}$ est d√©sormais parfaitement triangulaire sup√©rieure. C'est notre $U$.
$$
U = \begin{pmatrix} 2 & 1 & 1 \\ 0 & -1 & -2 \\ 0 & 0 & -4 \end{pmatrix}
$$
La matrice $L$ est la matrice triangulaire inf√©rieure form√©e d'une diagonale de $1$ et des trois multiplicateurs calcul√©s aux √©tapes pr√©c√©dentes ($l_{21}=2$, $l_{31}=-1$, $l_{32}=-3$) pos√©s exactement √† leur position d'action :
$$
L = \begin{pmatrix} 1 & 0 & 0 \\ 2 & 1 & 0 \\ -1 & -3 & 1 \end{pmatrix}
$$
Vous pouvez v√©rifier sur papier que $L \cdot U = A$ !
</details>

---

**Exercice 18 ‚Äî R√©solution en 2 Temps avec $LU$**

Maintenant que le dur travail est fait ($A$ a √©t√© factoris√© en $L \cdot U$ en perdant $\frac{2}{3}n^3$ flops CPU), comment l'ordinateur r√©sout l'√©quation complexe $Ax = b$ de mani√®re √©clair ? R√©digez le processus informatis√© en 2 temps de $\sim O(n^2)$.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

On substitue dans l'√©quation :
$$ L \cdot U \cdot x = b $$

L'ordinateur d√©finit astucieusement un vecteur interm√©diaire in-memory $y$ et proc√®de en deus temps :
1. **Descente (Forward substitution) :** Il r√©sout $L \cdot y = b$ (co√ªt $n^2$).
2. **Remont√©e (Backward substitution) :** Il r√©sout la partie de droite $U \cdot x = y$ pour trouver enfin $x$ brut (co√ªt $n^2$).

C'est √ßa la gloire de LU ! Si l'utilisateur me donne un nouveau vecteur $b'$, je n'ai plus besoin de refaire l'√©norme factorisation $A$, je n'ex√©cute que mes 2 descentes ultra-rapides.
</details>

---

### ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Niveau 5 ‚Äî Le Pivotage et Cas Extr√™mes

---

**Exercice 19 ‚Äî La Ruine Totale de $LU$ et la Matrice de Permutation**

La factorisation $LU$ standard sans modification **tombe en ruine absolue** sur le syst√®me trivial suivant :
$$
\begin{pmatrix} 10^{-20} & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} 1 \\ 2 \end{pmatrix}
$$
Expliquez pourquoi le calcul machine crashe, et pr√©sentez la m√©thode absolue requise (impliquant une Matrice $P$) : **Le Pivotage**.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

**Pourquoi √ßa crache en $LU$ pur :**
La matrice choisit imp√©rativement la coordonn√©e $(1, 1)$ comme pivot pour √©craser la ligne 2.
Le multiplicateur exig√© sera donc $\frac{\text{Ligne 2}}{\text{Ligne 1}} = \frac{1}{10^{-20}} = 10^{20} !$. 
En op√©rant sur l'ordinateur la ligne $2$ :
$$ L_2 = L_2 - 10^{20} L_1 \implies (1) - 10^{20}(1) = \text{Catastrophe d'arrondi totale} $$
Le $1$ est d√©truit, l'erreur relative propulse et tout le logiciel explose.

**Le Sauveur (Pivotage Partiel $PA = LU$) :**
L'algorithme moderne ne fonce pas t√™te baiss√©e. √Ä chaque colonne de travail, il regarde vers le bas toutes les lignes, s√©lectionne **la plus grande valeur absolue**, et √©change g√©om√©triquement l'ordre de TOUTE la ligne de la matrice. L'ordinateur m√©morise cet √©change dans une matrice de permutation unitaire modifi√©e $P$.

L'algorithme parfait et inconditionnellement stable de l'Analyse Num√©rique n'est donc pas $A=LU$, mais **$PA = LU$**.
</details>

---

**Exercice 19/Bis ‚Äî L'Astuce de $\det(A)$**

En vous appuyant du th√©or√®me des d√©terminants $\det(AB) = \det(A)\det(B)$, et de la sp√©cificit√© de la diagonale de $L, U$ et $P$; d√©montrez pourquoi Octave/Matlab peut calculer le d√©terminant d'une matrice gigantesque instantan√©ment d√®s que $PA = LU$ est termin√©.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

Si $PA=LU$, alors $\det(P)\det(A) = \det(L)\det(U)$.

1. $L$ et $U$ sont purement triangulaires, donc leur d√©terminant est exactement le produit de leur diagonale. 
2. Magie : $L$ n'a QUE des $1$ sur sa propre diagonale. Donc $\det(L) = 1$. Toujours !
3. $P$ est une matrice de permutation, √† chaque √©change de ligne le signe de $\det(P)$ s'inverse depuis $+1$. Ainsi, $\det(P) = (-1)^p$ avec $p$ le nombre de pivots invers√©s.

L'√©quation finale g√©niale :
$$
\det(A) = (-1)^p \cdot \det(U) = (-1)^p \cdot u_{11} \cdot u_{22} \cdots u_{nn}
$$
Il suffit √† la machine de multiplier la simple diagonale existante de $U$ !
</details>

---

**Exercice 19/Bis ‚Äî L'Astuce de $\det(A)$**

En vous appuyant du th√©or√®me des d√©terminants $\det(AB) = \det(A)\det(B)$, et de la sp√©cificit√© de la diagonale de $L, U$ et $P$; d√©montrez pourquoi Octave/Matlab peut calculer le d√©terminant d'une matrice gigantesque instantan√©ment d√®s que $PA = LU$ est termin√©.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

Prenons la base : $PA = LU$
$$
\det(PA) = \det(LU) \implies \det(P)\det(A) = \det(L)\det(U)
$$

Regardons les propri√©t√©s incroyables des actants :
- **$P$** : C'est une matrice d'Identit√© o√π les lignes ont √©t√© interchang√©es $p$ fois. Donc $\det(P) = (-1)^p$.
- **$L$** : Matrice triangulaire. Son d√©terminant est produit de sa diagonale. Son principe d'existence exige des $1$ absolus sur sa diagonale. Donc $\det(L) = 1$.
- **$U$** : Matrice triangulaire. Son d√©terminant est aussi le produit de sa diagonale (les pivots ultimes : $u_{11}, u_{22}, \dots, u_{nn}$).

Injectons tout √ßa dans l'√©quation :
$$
(-1)^p \cdot \det(A) = 1 \cdot \prod u_{ii} \implies \det(A) = (-1)^p \cdot (u_{11} \cdot u_{22} \cdots u_{nn})
$$

**Conclusion magique :** D√®s que Gauss a ex√©cut√© son $PA = LU$ (qui de toutes fa√ßons est n√©cessaire pour plein d'autres choses), le d√©terminant complet du syst√®me originel se r√©v√®le par simple multiplication de l'ossature diagonale de $U$ !
</details>

---

## Chapitre 3 : Factorisation QR et syst√®mes surd√©termin√©s

> üìà **Difficult√© croissante :** De la compr√©hension de l'orthogonalit√© (‚≠ê) √† la preuve absolue de la sup√©riorit√© algorithmique de QR sur le mod√®le $A^TA$ (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê). Pr√©parez vos d√©monstrations g√©om√©triques !

---

### ‚≠ê Niveau 1 ‚Äî Matrices Orthogonales et Factorisation QR

---

**Exercice 20 ‚Äî La D√©composition Formelle**

Expliquez avec des mots ce qu'est la factorisation QR compl√®te d'une matrice $A$ de dimension $m \times n$ (avec $m \geq n$). √Ä quoi ressemblent physiquement les matrices $Q$ et $R$ ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

C'est une scission de $A$ en deux composants : $A = QR$.
- **La matrice $Q$** est carr√©e ($m \times m$) et **orthogonale**. Toutes ses colonnes sont des vecteurs de longueur 1 qui sont strictement perpendiculaires les uns aux autres. $Q^{-1} = Q^T$.
- **La matrice $R$** est un grand rectangle ($m \times n$) **trap√©zo√Ødal sup√©rieur**. Au lieu d'avoir un "triangle", toute la partie inf√©rieure sous la diagonale principale est remplie de z√©ros. √âtant donn√© que $m \geq n$, tout le grand bloc inf√©rieur final de $m-n$ lignes n'est compos√© que d'√©tages de z√©ros. De ce fait, on l'ampute souvent informatiquement. C'est la factorisation QR r√©duite $\hat{Q}\hat{R} = A$.
</details>

---

### ‚≠ê‚≠ê Niveau 2 ‚Äî Les Miroirs de Householder

---

**Exercice 21 ‚Äî D√©monstration de l'Inversion de la matrice $H$**

Soit $v$ un vecteur r√©el de rebond. La matrice de transformation de Householder s'√©crit $H = I - 2 \frac{vv^T}{\|v\|^2_2}$.
**D√©montrez math√©matiquement** que cette matrice est strictement orthogonale (Prouvez formellement que $H^T H = I$).

*Indice : Souvenez-vous que $(vv^T)(vv^T) = v \|v\|^2 v^T$.*

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

La d√©monstration alg√©brique pas √† pas :
Premi√®rement, la matrice est visuellement sym√©trique car $(vv^T)^T = (v)^T(v^T)^T = vv^T$. Ainsi $H^T = H$.
On veut √©valuer $H^T H$. Comme elle est sym√©trique, cela revient √† calculer $H \cdot H$ (soit $H^2$) :

$$
HH = \left(I - 2 \frac{vv^T}{\|v\|_2^2}\right) \left(I - 2 \frac{vv^T}{\|v\|_2^2}\right)
$$
On distribue cette double parenth√®se :
$$
= I^2 - 2 \frac{vv^T}{\|v\|_2^2} - 2 \frac{vv^T}{\|v\|_2^2} + 4 \frac{(vv^T)(vv^T)}{(\|v\|_2^2)^2}
$$
On fusionne les deux termes du milieu :
$$
= I - 4 \frac{vv^T}{\|v\|_2^2} + 4 \frac{v (v^Tv) v^T}{\|v\|_2^4}
$$
Astuce matricielle d√©cisive : la quantit√© $(v^T v)$ au milieu du dernier num√©rateur est la d√©finition exacte du produit scalaire, donc de la norme au carr√© $\|v\|_2^2$.
$$
= I - 4 \frac{vv^T}{\|v\|_2^2} + 4 \frac{v \|v\|_2^2 v^T}{\|v\|_2^4}
$$
Le scalaire $\|v\|_2^2$ en haut s'annule avec une puissance du dominateur $\|v\|_2^4 \to \|v\|_2^2$ :
$$
= I - 4 \frac{vv^T}{\|v\|_2^2} + 4 \frac{vv^T}{\|v\|_2^2}
$$
Les deux √©normes fractions r√©siduelles sont strictement identiques de signes oppos√©s. Elles s'autod√©truisent :
$$
HH = I
$$
La matrice miroir est strictement orthogonale. C'est magique : son inverse est elle-m√™me ! (Faire le "miroir" deux fois de suite nous ram√®ne √† la position initiale g√©om√©trique).
</details>

---

### ‚≠ê‚≠ê‚≠ê Niveau 3 ‚Äî √âquations Normales (Les Moindres Carr√©s)

---

**Exercice 22 ‚Äî Trouver le Sommet du Crat√®re (Le Tenseur de Gradient)**

Votre algorithme industriel tente d'approcher un mod√®le de donn√©es bruit√© $Ax \approx b$. La cuvette de p√©nalit√© √† minimiser est $f(x) = \|b - Ax\|^2$.
**Construisez de A √† Z la d√©monstration alg√©brique** trouvant le point critique optimal (o√π $\nabla f(x) = 0$) pour d√©couvrir le syst√®me supr√™me des Equations Normales $A^T A x = A^T b$.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

**D√©monstration alg√©brique :**
D√©veloppons la norme au carr√© de l'erreur en multiplications matricielles classiques ($v^Tv$) :
$$ 
f(x) = (b-Ax)^T(b-Ax) 
$$
Distribuons la transpos√©e $(AB)^T = B^TA^T$ :
$$ 
f(x) = (b^T - x^TA^T)(b-Ax) 
$$
Distribuons compl√®tement la double parenth√®se :
$$ 
f(x) = b^Tb - b^TAx - x^TA^Tb + x^TA^TAx 
$$
Les deux termes du milieu sont des scalaires (produits purs $1 \times 1$). Un scalaire est √©gal √† sa propre transpos√©e, donc $(b^TAx)^T = x^TA^Tb$. Par cons√©quent, ces deux termes sont identiques et on peut les fusionner :
$$ 
f(x) = b^Tb - 2(A^Tb)^Tx + x^T(A^TA)x 
$$
Calculons la jacobienne (d√©riv√©e multi-variable vectorielle par rapport au vecteur colonne $\vec{x}$) :
- La d√©riv√©e d'une constante pure ($b^Tb$) est la matrice $\mathbf{0}$.
- La d√©riv√©e vectorielle d'un tenseur lin√©aire projet√© $-2(c)^Tx$ est formellement $-2c \implies -2A^Tb$.
- La d√©riv√©e de la forme quadratique centrale $x^T(\text{Sym√©trique})x$ devient $2(\text{Sym√©trique})x \implies 2A^TAx$.
La jacobienne totale est $\nabla f = -2A^Tb + 2A^TAx$.

Pour trouver le minimum de la cuvette, on pose $\nabla f = \mathbf{0}$ :
$$ 
-2A^Tb + 2A^TAx = \mathbf{0} \implies 2A^TAx = 2A^Tb \implies \boxed{A^TAx = A^Tb} 
$$
Ce magnifique syst√®me est le r√©seau des √âquations Normales.
</details>

---

**Exercice 23 ‚Äî L'Angle d'Improvisation $\theta$**

Si l'ordinateur fait de son mieux pour limiter l'erreur $r = b - Ax_{approx}$, √† quoi correspond g√©om√©triquement l'angle $\theta$ ? Pourquoi veut-on qu'il tende vers z√©ro absolu ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

L'angle $\theta$ s√©pare le nuage absolu des donn√©es brutes r√©elles (le vecteur abstrait $b$) et la fine plaque d'approximation lin√©aire construite par notre mod√®le (l'hyperplan construit par l'image $\text{Im}(A)$ de nos algorithmes).
- Si l'angle est √† $0$, le vecteur vrai repose PARFAITEMENT sur la plaque des pr√©dictions. L'erreur de notre mod√®le est formellement nulle ($r = 0$). Les observations collent √† la th√©orie √† la perfection.
- Si l'angle pointe vers $90^\circ$, cela signifie que notre mod√®le s'enfonce dans une dimension spatiale o√π la donn√©e r√©elle de $b$ diverge orthogonalement... L'erreur de l'approximation $r$ va √©craser la pr√©diction pure, l'hypoth√®se (notre grille A) est absolument impuissante √† mod√©liser la donn√©e physique $b$.
</details>

---

### ‚≠ê‚≠ê‚≠ê‚≠ê Niveau 4 ‚Äî Complexit√© Algorithmique

---

**Exercice 24 ‚Äî La guerre du CPU : Forme brute vs Forme Orthogonale**

√ânoncez les √©tapes informatiques (et leur temps Processeur respectif en flops) pour r√©soudre un r√©seau surd√©termin√© $m \times n$ (avec $m \gg n$). D'un c√¥t√© par la formation de base brute des √©quations normales, de l'autre point de vue par la factorisation QR stricte des matrices. Quelle m√©thode consomme le double de son adversaire ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

**Option 1 : M√©thodes des √âquations Normales (LU brut) :**
- Assembler le super bloc sym√©trique $A^T A$ (et modifier $b \to A^Tb$) : Frappe processeur extr√™mement massive car on multiplie deux grandes matrices rectangulaires $\to \approx m n^2$ flops.
- Attaquer le bloc g√©n√©r√© avec la factorisation $LU$ pivot√©e au centre de son ar√®ne (bloc $n \times n$) $\to \approx \frac{2}{3}n^3$ flops.
- Total domin√© par : $\approx mn^2$ flops.

**Option 2 : M√©thode QR par Miroirs Householder :**
- Calcul de la factorisation formelle directe sans jamais d√©truire les matrices originales $A = QR$ avec une multitude de matrices miroirs $\to \approx 2n^2(m - \frac{n}{3})$. C'est massif car on attaque physiquement toute la hauteur $m$.
- Lancer le vecteur de substitution terminal $Rx = Q^Tb$ (Trivial et gratuit) $\to \approx 4mn$ flops.
- Total domin√© par : $\approx 2mn^2$ flops.

**Vainqueur (Vitesse Brute) :** La victoire CPU revient √©crasamment aux √âquations Normales simples, exigeant litt√©ralement la **moiti√© du temps de calcul total** par rapport √† l'extraction fine g√©om√©trique $QR$. L'ordinateur pr√©f√®re de loin le $LU$ basique. (Mais √† quel prix algorithmique... ? Voir niveau 5 !)
</details>

---

### ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Niveau 5 ‚Äî Le D√©sastre de Stabilit√© $\kappa(A)^2$

---

**Exercice 25 ‚Äî Pourquoi les statisticiens v√©n√®rent QR**

La matrice carr√©e $A^TA$ est magique √† manipuler, la m√©thode des √©quations usuelles gagne tout niveau rapidit√© (voir dessus)... Et pourtant, l'algorithme $QR$ est vital. 
**Prouvez math√©matiquement** ce qu'il se passera concernant l'instabilit√© (Le Conditionnement $\kappa$) du pauvre processeur tentant de r√©soudre une matrice complexe $A$ si elle a la malchance d'√™tre fortement instable au d√©part (ex: $\kappa(A) = 1.0 \times 10^{11}$). 

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

Pour comprendre pourquoi l'ordinateur qui utilise $LU$ sur $A^T A$ va crasher ses m√©moires, nous devons v√©rifier √† quel conditionnement math√©matique la factorisation fait face. LU ne va pas se heurter √† $\kappa(A)$, il va se heurter √† l'hydre **$\kappa(A^T A)$** !

Analysons la formule absolue en norme :
$$ 
\kappa(A^T A) = \| (A^T A)^{-1} \|_2 \cdot \| A^T A \|_2 
$$
Souvenons nous des interludes math√©matiques sur la norme des transpos√©es sym√©triques : $\| A^T A \|_2 = \|A\|^2_2$.  
Il s'av√®re que cela fonctionne pareil pour l'pseudo inverse :
$$ 
\kappa(A^T A) = \| A^{\dagger} \|^2_2 \cdot \|A\|^2_2 = (\kappa(A))^2 
$$

**La R√©alit√© Informatique du Crash :**
Le processeur affronte litt√©ralement avec fureur **$\kappa(A)$ √©lev√© au carr√©** :
Si la matrice de d√©part avait un conditionnement de $10^{11}$, l'ordinateur, pour pouvoir s'ex√©cuter √† la va-vite, fabrique au milieu de sa m√©moire RAM une monstruosit√© absolue conditionn√©e √† $10^{22}$ !!
Sachant que la limitation physique absolue des doubles pr√©cisions IEEE-754 ($64$ bits) ne m√©morise que 16 chiffres stricts ($\approx 10^{-16}$). Les erreurs microscopiques in√©vitables vont exploser par un levier multiplicateur massif $10^{22}$. 

La totalit√© des d√©cimales du r√©sultat sortant $\sim x$ seront compos√©es exclusivement avec **$100\%$ de bruit blanc d'arrondis sans aucun lien avec l'alg√®bre**. C'est une perte s√®che !

**Conclusion :** Householder et sa douce Factorisation Orthogonale pure $A=QR$ n'utilise **jamais** la structure en carr√© $A^T A$. La chirurgie op√®re de face et encaisse simplement $\approx \kappa(A)$, sauvant formellement des milliards de calculs de la destruction informatique au profit de math√©matiques fiables et robustes.
</details>


