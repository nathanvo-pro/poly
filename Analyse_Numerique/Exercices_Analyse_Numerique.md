# Exercices ‚Äî Analyse Num√©rique

## Chapitre 1 : Repr√©sentation en virgule flottante, stabilit√© et conditionnement

> üìà **Difficult√© croissante :** Les exercices vont des d√©finitions de base (‚≠ê) aux probl√®mes de r√©flexion math√©matique (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê). Pensez √† chercher sur papier avant d'ouvrir les solutions !

---

### ‚≠ê Niveau 1 ‚Äî Fondamentaux de la virgule flottante

---

**Exercice 1 ‚Äî Vocabulaire**

Dans l'expression standard $\pm 0.d_1 d_2 \cdots d_t \cdot \beta^e$, √† quoi correspondent exactement les lettres $t$, $\beta$, $e$ et $d_i$ ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

- $\beta$ : La base du syst√®me (binaire $\beta=2$ ou d√©cimale $\beta=10$)
- $t$ : Le nombre exact de chiffres significatifs (qui dicte la pr√©cision)
- $e$ : L'exposant (qui dicte l'ordre de grandeur, encadr√© par $e_{\min}$ et $e_{\max}$)
- $d_i$ : Le $i$-√®me chiffre (encadr√© par $0 \le d_i \le \beta - 1$)

L'ensemble des chiffres forme ce qu'on appelle la **mantisse**.
</details>

---

**Exercice 2 ‚Äî Repr√©sentation Normalis√©e**

Qu'est-ce qu'une mantisse normalis√©e ? Pourquoi est-ce un avantage massif pour les syst√®mes binaires ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

Une repr√©sentation est **normalis√©e** si son tout premier chiffre significatif est non nul ($d_1 \neq 0$). 
Cela permet d'avoir une repr√©sentation unique pour chaque nombre (par exemple, on interdit l'ambigu√Øt√© entre $2 = 0.2 \cdot 10^1 = 0.02 \cdot 10^2$).

**L'avantage massif en binaire :** Si la base est $2$, les seuls chiffres possibles sont $0$ et $1$. Puisque $d_1$ ne peut pas √™tre $0$, **il est obligatoirement √©gal √† $1$**. Puisqu'on le sait toujours √† l'avance, on n'a plus besoin de le stocker en m√©moire ! Cela fait gagner $1$ bit de pr√©cision gratuitement (le bit implicite).
</details>

---

**Exercice 3 ‚Äî D√©monstration de l'Unit√© d'arrondi $u$**

√ânoncez la formule de l'unit√© d'arrondi $u$ relative √† la virgule flottante. Ensuite, **d√©montrez math√©matiquement** cette borne sup√©rieure √† partir de la distance absolue maximale induite par l'arrondi du $(t+1)$-√®me chiffre.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

L'unit√© d'arrondi r√©pond √† la formule :
$$
u = \frac{1}{2}\beta^{1-t}
$$

**D√©monstration formelle :**
Soit un r√©el strict $x = \pm 0.d_1 d_2 \cdots d_t d_{t+1} \cdots \cdot \beta^e$.
Son √©quivalent machine $\text{fl}(x)$ ne garde que $t$ chiffres. L'arrondi ("au plus proche") d√©pend du chiffre $d_{t+1}$. L'√©cart absolu maximal entre la vraie valeur et le flottant machine ne d√©passera jamais la moiti√© du poids du dernier chiffre (le $t$-i√®me chiffre) :
$$
|x - \text{fl}(x)| \leq \frac{1}{2} \beta^{-t} \cdot \beta^e
$$
Ensuite, on cherche l'erreur relative maximale, on divise donc tout par $|x|$.
La plus petite valeur que peut prendre la mantisse de $|x|$ (condition critique) est $1.00\dots \times \beta^{-1}$ (puisque $d_1 \geq 1$).
Donc $|x|_{\min} = \beta^{-1} \cdot \beta^e = \beta^{e-1}$.
Divisons :
$$
\text{Erreur relative max} = \frac{|x - \text{fl}(x)|}{|x|} \leq \frac{\frac{1}{2} \beta^{-t} \beta^e}{\beta^{e-1}} = \frac{1}{2}\beta^{1-t}
$$
Cette borne universelle d'erreur s'appelle $u$.
</details>

---

### ‚≠ê‚≠ê Niveau 2 ‚Äî Le Standard IEEE et l'Erreur Relative

---

**Exercice 4 ‚Äî Erreur absolue vs Erreur relative**

Si je souhaite stocker la valeur absolue $A = 1000$ et qu'elle devient $\hat{A} = 1000.5$, calculez l'erreur absolue puis l'erreur relative.
Recommencez si je veux stocker la valeur $B = 0.001$ et qu'elle devient $\hat{B} = 0.0015$. Quelle conclusion en tirer ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

**Pour A ($1000 \to 1000.5$) :**
- Erreur absolue = $|1000.5 - 1000| = 0.5$
- Erreur relative = $0.5 / 1000 = 0.0005$ soit $\approx 0.05\%$

**Pour B ($0.001 \to 0.0015$) :**
- Erreur absolue = $|0.0015 - 0.001| = 0.0005$
- Erreur relative = $0.0005 / 0.001 = 0.5$ soit $\approx 50\%$

**Conclusion :** 
Ici l'erreur absolue sur $B$ est de seulement $0.0005$ (ce qui semble insignifiant), mais proportionnellement √† sa petite grandeur, on se trompe de $50\%$ ! L'erreur relative est la seule m√©trique fiable en analyse num√©rique pour √©valuer l'impact r√©el d'une impr√©cision.
</details>

---

**Exercice 5 ‚Äî Les limites (Overflow, Underflow)**

Que devient l'√©tat du processeur si l'on calcule :
1. $2 \cdot x_{\max}$
2. $x_{\min} / 2$
3. $0.0 / 0.0$

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

1. **Overflow (d√©passement sup√©rieur) :** La valeur d√©passe le maximum stockable pour l'exposant. Le standard dicte de renvoyer la valeur sp√©ciale `Inf` (Infini).
2. **Underflow (d√©passement inf√©rieur) :** La valeur devient plus petite que nos crans d'√©chelle. Cependant, plut√¥t que de crasher vers $0$, on passe √† une repr√©sentation **d√©normalis√©e**, mais en sacrifiant la pr√©cision (l'unit√© $u$ n'est plus garantie).
3. **NaN (Not a Number) :** Forme ind√©termin√©e qui ne donne aucun sens math√©matique.
</details>

---

### ‚≠ê‚≠ê‚≠ê Niveau 3 ‚Äî Propagation et Annulation

---

**Exercice 6 ‚Äî Le Mod√®le d'Arithm√©tique Standard**

Dites comment la machine calcule math√©matiquement l'op√©ration abstraite "$\circledcirc$" (qui remplace l'op√©ration math√©matique exacte $\circ$) entre deux flottants $A$ et $B$.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

Selon la sp√©cification IEEE, le processeur ex√©cute l'op√©ration exacte puis s'autorise une impr√©cision d'arrondi sur ce r√©sultat :

$$
A \circledcirc B = (A \circ B)(1 + \varepsilon), \quad |\varepsilon| \leq u
$$

Cette formule universelle permet de pr√©dire de combien s'amplifiera l'erreur relative apr√®s des centaines de multiplications/soustractions.
</details>

---

**Exercice 7 ‚Äî D√©monstration formelle de l'Annulation Catastrophique ($\kappa$)**

Montrez un exemple chiffr√© basique o√π la soustraction de nombres an√©antit la pr√©cision $u$.
Ensuite, utilisez la formule du Conditionnement d'un sous-probl√®me diff√©rentiable $\kappa(x)$ pour **prouver math√©matiquement** que la soustraction de deux nombres extr√™mement proches provoque une explosion exponentielle des erreurs initiales vers l'infini.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

**1. L'Exemple chiffr√© (Le massacre de la mantisse) :**
Imaginons un syst√®me qui ne garde que $6$ chiffres significatifs exacts (comme la simple pr√©cision $\approx$ 7 chiffres).
Soit $x = 1.000000$ et $y = 0.999999$. Les deux sont tr√®s pr√©cis.
Soustrayons :
$$
x - y = 0.000001
$$
Le r√©sultat $\hat{y}$ est normalis√© par la machine (puisqu'une mantisse ne peut pas commencer par des z√©ros). Elle le d√©cale comme $1.00000 \cdot 10^{-6}$. Cependant, √† cause de l'exposant n√©gatif, la machine **comble le vide de derri√®re avec du bruit num√©rique al√©atoire latent** (car la soustraction a d√©truit les seuls chiffres rigoureusement connus). 

**2. La Preuve math√©matique par le conditionnement :**
Posons la fonction $f(x) = x_1 - x_2$.
Son vecteur de d√©riv√©es partielles est $f'(x) = [1, -1]$. On prend la norme euclidienne $\|f'(x)\|_2 = \sqrt{1^2 + (-1)^2} = \sqrt{2}$.

Ins√©rons dans la formule jacobienne de $\kappa$ locale :
$$
\kappa(\text{soustraction}) = \frac{\|f'(x)\| \cdot \|x\|}{|f(x)|} = \frac{\sqrt{2} \cdot \sqrt{x_1^2 + x_2^2}}{|x_1 - x_2|}
$$

**Conclusion finale :** Si nos valeurs sont proches ($x_1 \approx x_2$), le d√©nominateur de la fraction tend furieusement vers $0$, tandis que le num√©rateur reste hautement positif (les carr√©s).
Cons√©quence indiscutable de l'alg√®bre : $\kappa \to \infty$. 
Le conditionnement du probl√®me devient dramatique, la moindre impr√©cision machine d'entr√©e explosera d'un facteur immense dans la sortie finale de la soustraction.
</details>

---

### ‚≠ê‚≠ê‚≠ê‚≠ê Niveau 4 ‚Äî Stabilit√© et Conditionnement

---

**Exercice 8 ‚Äî Stabilit√© algorihtmique (Directe et Inverse)**

D√©crivez avec des mots simples (et la formule de base avec $C$) la diff√©rence entre un algorithme qui b√©n√©ficie de la *stabilit√© directe* par rapport √† la *stabilit√© inverse*. Lequel implique directement l'autre ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

- **Stabilit√© Directe :** Mon algorithme se trompe l√©g√®rement ($\hat{y} - y$ est petit). Mais rassurons-nous : sa marge d'erreur directe n'est *pas pire* que si j'avais donn√© des donn√©es minusculement perturb√©es √† une fonction parfaite.
- **Stabilit√© Inverse :** C'est un concept encore plus solide. Il dit : Mon r√©sultat inexact et imparfait ($\hat{y}$) N'EST PAS juste "proche", il **EST** LA r√©ponse parfaite et math√©matique √† un probl√®me ($\Delta x$) qui fr√¥le le mien de tr√®s pr√®s ($\leq u$).

$$
\text{Stabilit√© Inverse} : \quad f(x + \Delta x) = \hat{y} \quad \text{avec } \frac{\|\Delta x\|}{\|x\|} \leq C \cdot u
$$

**Conclusion :** La stabilit√© inverse est beaucoup plus robuste. C'est elle qui **implique et entra√Æne automatiquement** la stabilit√© directe.
</details>

---

**Exercice 9 ‚Äî Calcul du Conditionnement $\kappa(x)$**

Quelle est la formule officielle pour mesurer le conditionnement d'un probl√®me avec une fonction diff√©rentiable ? Prouvez que le calcul de la racine carr√©e $f(x) = \sqrt{x}$ est extr√™mement bien conditionn√© num√©riquement, sans lien avec l'algorithme choisi.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

La formule utilisant la Jacobienne / d√©riv√©e est :

$$
\kappa(x) = \frac{\|f'(x)\| \cdot \|x\|}{\|f(x)\|}
$$

Appliqu√© √† notre racine carr√©e $f(x) = x^{1/2}$ et donc $f'(x) = \frac{1}{2\sqrt{x}}$ :

$$
\kappa(x) = \frac{\frac{1}{2\sqrt{x}} \cdot x}{\sqrt{x}} = \frac{\frac{x}{2\sqrt{x}}}{\sqrt{x}} = \frac{1}{2}
$$

Ici $\kappa(x) = 1/2$. √âtant donn√© que le conditionnement est aux alentours de $1$ (et encore mieux : il pond√®re/r√©duit l'erreur), ce probl√®me physique tol√®re excessivement bien l'impr√©cision et ne causera aucun comportement chaotique ! C'est remarquablement bien conditionn√©.
</details>

---

### ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Niveau 5 ‚Äî Probl√®mes Num√©riques Complexes

---

**Exercice 10 ‚Äî Le Pi√®ge de la Soustraction Analys√© via $\kappa(x)$**

En utilisant la formule du conditionnement $\kappa(x)$ ci-dessus, prouvez math√©matiquement pourquoi la fonction de soustraction $f(x_1, x_2) = x_1 - x_2$ devient dramatiquement instable (Conditionnement vers l'infini) d√®s que $x_1$ et $x_2$ se rapprochent tr√®s fort. 

(Utilisez la norme vectorielle euclidienne $\|x\| = \sqrt{x_1^2 + x_2^2}$).

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

Notre fonction √† deux variables est $f(x_1, x_2) = x_1 - x_2$.
Le gradient (la d√©riv√©e) par rapport aux composantes donne le vecteur $f'(x) = (1, -1)$.
La norme de ce gradient est $\|(1, -1)\| = \sqrt{1^2 + (-1)^2} = \sqrt{2}$.

Appliquons la formule du conditionnement pour multivariables :

$$
\kappa(x_1, x_2) = \frac{\|f'(x)\| \cdot \|(x_1, x_2)\|}{\|f(x_1, x_2)\|}
$$

En ins√©rant la norme du gradient ($\sqrt{2}$), la norme des entr√©es ($\sqrt{x_1^2 + x_2^2}$) et la valeur absolue formelle de la fonction au d√©nominateur :

$$
\kappa(x_1, x_2) = \frac{\sqrt{2} \cdot \sqrt{x_1^2 + x_2^2}}{|x_1 - x_2|}
$$

**Que se passe-t-il lorsque $x_1 \approx x_2$ ?**
Le num√©rateur reste strictement positif (car les carr√©s s'additionnent et valent en gros $2 \cdot x_1^2$).
Mais le d√©nominateur $|x_1 - x_2|$ **tend farouchement vers z√©ro** !
Un nombre positif divis√© par presque-z√©ro explose vers l'infini. 
Donc : **$\kappa(x_1 \approx x_2) \to \infty$**. 

Le petit $\Delta x$ d'incertitude dans l'ordinateur se verra multipli√© par un million ou un milliard en franchissant cette op√©ration. Le probl√®me est horriblement **mal conditionn√©** !
</details>

---

## Chapitre 2 : Syst√®mes d'√©quations lin√©aires : m√©thodes directes

> üìà **Difficult√© croissante :** De la compr√©hension des normes (‚≠ê) √† la manipulation matricielle experte sur la factorisation PA=LU (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê). Pensez √† l'alg√®bre lin√©aire !

---

### ‚≠ê Niveau 1 ‚Äî Fondamentaux du Conditionnement $\kappa(A)$

---

**Exercice 11 ‚Äî La formule de $\kappa(A)$**

Donnez la d√©finition math√©matique du conditionnement d'une matrice $A$ r√©guli√®re, not√© $\kappa(A)$. Que signifie physiquement un conditionnement de $1$ ? Et un conditionnement de $10^{15}$ ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

La d√©finition officielle du conditionnement matriciel est :

$$
\kappa(A) = \|A^{-1}\| \cdot \|A\|
$$

- Si $\kappa(A) \approx 1$ : Le syst√®me est excellemment bien conditionn√©. Une petite erreur dans les donn√©es (le vecteur $b$) entra√Ænera une erreur de la m√™me taille dans le r√©sultat (le vecteur $x$).
- Si $\kappa(A) \approx 10^{15}$ : Le syst√®me est catastrophiquement mal conditionn√©. La moindre d√©cimale fausse dans l'√©quation de base sera amplifi√©e $10^{15}$ fois dans le r√©sultat final ! C'est g√©n√©ralement le cas de matrices presque singuli√®res.
</details>

---

**Exercice 12 ‚Äî Preuve du th√©or√®me d'amplification d'erreur**

Supposons le syst√®me exact $Ax = b$. √Ä cause de l√©g√®res d√©rives mat√©rielles, l'ordinateur r√©sout en r√©alit√© le syst√®me perturb√© $(A+\delta A)(x+\delta x) = (b + \delta b)$. Pour simplifier, assumons qu'il n'y a un bruit d'incertitude que sur le second membre analytique $b$ (donc $\delta A = 0$).

**D√©montrez math√©matiquement** (en utilisant les propri√©t√©s de l'inverse $A^{-1}$ et des normes $\leq$) le th√©or√®me d'amplification d'erreur qui prouve que l'erreur relative de la solution calcul√©e d√©pend d'un multiplicateur purement matriciel. Identifier le nom de ce modificateur.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

**Preuve √©tape par √©tape :**
Soit l'√©quation du syst√®me perturb√© uniquement en $b$ :
$$ A(x + \delta x) = b + \delta b $$
En distribuant, on a $Ax + A\delta x = b + \delta b$.
Or, on sait fondamentalement que la r√©ponse id√©ale est $Ax = b$. Ces termes purs s'annulent de chaque c√¥t√© :
$$ A \delta x = \delta b $$

L'ordinateur trouve son erreur r√©elle $\delta x$ (le parasite inject√©) en inversant la d√©pendance :
$$ \delta x = A^{-1} \delta b $$

On applique ensuite les normes. La propri√©t√© des normes induites donne $\|M v\| \leq \|M\| \|v\|$ :
$$ \|\delta x\| \leq \|A^{-1}\| \|\delta b\| \quad \text{(√âq. 1)} $$

Pour exprimer l'amplification *relative*, on doit diviser par la norme pure de la vraie solution $x$.
On sait que $Ax = b \implies \|b\| \leq \|A\| \|x\|$.
Inversons magiquement de bord cette in√©quation rigoureuse :
$$ \frac{1}{\|x\|} \leq \frac{\|A\|}{\|b\|} \quad \text{(√âq. 2)} $$

Multiplions intelligemment la partie gauche de l'√âq.1 par la partie gauche de l'√âq.2 :
$$
\frac{\|\delta x\|}{\|x\|} \leq \|A^{-1}\| \|\delta b\| \cdot \frac{\|A\|}{\|b\|}
$$

On r√©ordonne l'in√©quation en isolant le bloc pur des erreurs quantifi√©es :
$$
\frac{\|\delta x\|}{\|x\|} \le \bigl( \|A^{-1}\| \cdot \|A\| \bigr) \frac{\|\delta b\|}{\|b\|}
$$

**Conclusion :** 
L'amplification de l'erreur ne d√©pend **absolument que de la matrice de base**, peu importe l'algorithme informatique utilis√©.
Ce rapport s'appelle le Conditionnement de la Matrice $\kappa(A) = \|A^{-1}\| \cdot \|A\|$.
</details>

---

### ‚≠ê‚≠ê Niveau 2 ‚Äî Algorithmique des Matrices

---

**Exercice 13 ‚Äî Normes Vectorielles ($L_1, L_2, L_\infty$)**

Soit le vecteur $v = \begin{pmatrix} -3 \\ 1 \\ 4 \end{pmatrix}$. 
Calculez manuellement ses trois normes principales : $\|v\|_1$, $\|v\|_2$, et $\|v\|_\infty$.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

- **Norme 1 (Somme des valeurs absolues) :** 
  $\|v\|_1 = |-3| + |1| + |4| = 3 + 1 + 4 = 8$
- **Norme 2 (Euclidienne, racine de la somme des carr√©s) :** 
  $\|v\|_2 = \sqrt{(-3)^2 + 1^2 + 4^2} = \sqrt{9 + 1 + 16} = \sqrt{26} \approx 5.1$
- **Norme infini (Le max absolu) :** 
  $\|v\|_\infty = \max(|-3|, |1|, |4|) = 4$
</details>

---

**Exercice 14 ‚Äî Les mauvaises pratiques ($n!$ vs $O(n^3)$)**

Pourquoi, pour r√©soudre informatiquement $Ax = b$, est-il rigoureusement interdit d'utiliser la **m√©thode de Cramer** (calcul des d√©terminants successifs) ou de **calculer formellement l'inverse $A^{-1}$** ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

- La m√©thode de Cramer exige un grand nombre de d√©terminants lourds. R√©soudre un syst√®me de taille $n$ prendrait $\approx n!$ op√©rations. (*Ex : Une matrice $100 \times 100$ prendrait des milliards d'ann√©es √† l'ordinateur le plus puissant du monde !*).
- Calculer explicitement l'inverse $A^{-1}$ avant de multiplier par $b$ implique de faire $3$ fois trop de calculs inutiles pour rien. L'inversion matricielle prend au minimum $\approx \frac{8}{3}n^3$ flops, alors qu'une m√©thode directe et asym√©trique asym√©trique (comme LU) ne prend que $\approx \frac{2}{3}n^3$ flops. L'inverse prend **$2.3 \times$ plus de temps** √† un processeur moderne sans aucun gain en pr√©cision.
</details>

---

### ‚≠ê‚≠ê‚≠ê Niveau 3 ‚Äî M√©thode de Gauss et Syst√®mes Triangulaires

---

**Exercice 15 ‚Äî La b√©n√©diction des Matrices Triangulaires**

Pourquoi l'analyse num√©rique r√©duit-elle syst√©matiquement ses grilles √† des **matrices triangulaires** (Inf√©rieures $L$ ou Sup√©rieures $U$) ? Quelle est la complexit√© algorithmique pour r√©soudre $Ux = y$ ?

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

Un syst√®me triangulaire comme $Ux = y$ a la d√©licieuse propri√©t√© d'avoir d√©j√† sa derni√®re ligne compl√®tement r√©solue ! (ex: $a_{nn}x_n = y_n \implies x_n = y_n/a_{nn}$).
Il suffit d'isoler $x_n$ puis d'injecter la r√©ponse dans la ligne au dessus, en remontant (C'est la m√©thode de *Substitution arri√®re* / *Backward substitution*).

Ce processus est :
1. Extr√™mement rapide : Seulement $n^2$ flops (Op√©rations polynomiales de degr√© $2$, donc trivial pour un ordinateur).
2. Toujours math√©matiquement rigoureux et stable, car aucune annulation catastrophique massive n'a lieu.
</details>

---

**Exercice 16 ‚Äî Transformation de Gauss en Matrice**

L'√©limination classique de Gauss consiste √† soustraire multiple d'une ligne d'une autre (ex: $L_2 = L_2 - (\frac{a_{21}}{a_{11}})L_1$). 
En Analyse Num√©rique, comment mod√©lise-t-on cela sous le spectre de l'Alg√®bre ? Que devient le syst√®me $A$ √† la fin de la proc√©dure ? 

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

On mod√©lise chaque annulation de Gauss non pas comme une manipulation externe, mais intrins√®quement comme la multiplication de $A$ par une matrice √©l√©mentaire Inf√©rieure $L_1, L_2, \dots$

Apr√®s avoir inject√© plusieurs matrices $L_i$ (qui √©crasent la moiti√© bas-gauche pour y mettre des $0$), la matrice compl√®te d'origine **fusionne en un gigantesque triangle sup√©rieur**. On l'appelle $U$.

$$
(L_{n-1} \dots L_2 \cdot L_1) \cdot A = U
$$
</details>

---

### ‚≠ê‚≠ê‚≠ê‚≠ê Niveau 4 ‚Äî La Factorisation LU

---

**Exercice 17 ‚Äî Ex√©cution algorithmique de la Factorisation $A = LU$**

D√©montrez la puissance de Gauss algorithmique. D√©composez manuellement la matrice $A$ ci-dessous en ses facteurs $L$ et $U$ en d√©taillant imp√©rativement les √©tapes interm√©diaires (les duplicateurs $l_{ik}$).
$$
A = \begin{pmatrix} 2 & 1 & 1 \\ 4 & 1 & 0 \\ -2 & 2 & 1 \end{pmatrix}
$$

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

**√âtape 1 : Z√©ros sous le premier pivot ($a_{11} = 2$)**
Multiplicateur ligne 2 : $l_{21} = \frac{4}{2} = 2$
Multiplicateur ligne 3 : $l_{31} = \frac{-2}{2} = -1$

On applique $L_2 = L_2 - 2 L_1$ et $L_3 = L_3 - (-1) L_1$ :
$$
A^{(2)} = \begin{pmatrix} 2 & 1 & 1 \\ 0 & -1 & -2 \\ 0 & 3 & 2 \end{pmatrix}
$$
*Note : On range d√©j√† nos multiplicateurs ($2$ et $-1$) dans la 1√®re colonne de notre future matrice $L$ !*

**√âtape 2 : Z√©ro sous le deuxi√®me pivot ($a^{(2)}_{22} = -1$)**
Multiplicateur ligne 3 : $l_{32} = \frac{3}{-1} = -3$

On applique $L_3 = L_3 - (-3) L_2$ :
$$
A^{(3)} = \begin{pmatrix} 2 & 1 & 1 \\ 0 & -1 & -2 \\ 0 & 0 & -4 \end{pmatrix}
$$

**Finalisation Th√©orique :**
La matrice r√©sultante $A^{(3)}$ est d√©sormais parfaitement triangulaire sup√©rieure. C'est notre $U$.
$$
U = \begin{pmatrix} 2 & 1 & 1 \\ 0 & -1 & -2 \\ 0 & 0 & -4 \end{pmatrix}
$$
La matrice $L$ est la matrice triangulaire inf√©rieure form√©e d'une diagonale de $1$ et des trois multiplicateurs calcul√©s aux √©tapes pr√©c√©dentes ($l_{21}=2$, $l_{31}=-1$, $l_{32}=-3$) pos√©s exactement √† leur position d'action :
$$
L = \begin{pmatrix} 1 & 0 & 0 \\ 2 & 1 & 0 \\ -1 & -3 & 1 \end{pmatrix}
$$
Vous pouvez v√©rifier sur papier que $L \cdot U = A$ !
</details>

---

**Exercice 18 ‚Äî R√©solution en 2 Temps avec $LU$**

Maintenant que le dur travail est fait ($A$ a √©t√© factoris√© en $L \cdot U$ en perdant $\frac{2}{3}n^3$ flops CPU), comment l'ordinateur r√©sout l'√©quation complexe $Ax = b$ de mani√®re √©clair ? R√©digez le processus informatis√© en 2 temps de $\sim O(n^2)$.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

On substitue dans l'√©quation :
$$ L \cdot U \cdot x = b $$

L'ordinateur d√©finit astucieusement un vecteur interm√©diaire in-memory $y$ et proc√®de en deus temps :
1. **Descente (Forward substitution) :** Il r√©sout $L \cdot y = b$ (co√ªt $n^2$).
2. **Remont√©e (Backward substitution) :** Il r√©sout la partie de droite $U \cdot x = y$ pour trouver enfin $x$ brut (co√ªt $n^2$).

C'est √ßa la gloire de LU ! Si l'utilisateur me donne un nouveau vecteur $b'$, je n'ai plus besoin de refaire l'√©norme factorisation $A$, je n'ex√©cute que mes 2 descentes ultra-rapides.
</details>

---

### ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Niveau 5 ‚Äî Le Pivotage et Cas Extr√™mes

---

**Exercice 19 ‚Äî La Ruine Totale de $LU$ et la Matrice de Permutation**

La factorisation $LU$ standard sans modification **tombe en ruine absolue** sur le syst√®me trivial suivant :
$$
\begin{pmatrix} 10^{-20} & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} 1 \\ 2 \end{pmatrix}
$$
Expliquez pourquoi le calcul machine crashe, et pr√©sentez la m√©thode absolue requise (impliquant une Matrice $P$) : **Le Pivotage**.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

**Pourquoi √ßa crache en $LU$ pur :**
La matrice choisit imp√©rativement la coordonn√©e $(1, 1)$ comme pivot pour √©craser la ligne 2.
Le multiplicateur exig√© sera donc $\frac{\text{Ligne 2}}{\text{Ligne 1}} = \frac{1}{10^{-20}} = 10^{20} !$. 
En op√©rant sur l'ordinateur la ligne $2$ :
$$ L_2 = L_2 - 10^{20} L_1 \implies (1) - 10^{20}(1) = \text{Catastrophe d'arrondi totale} $$
Le $1$ est d√©truit, l'erreur relative propulse et tout le logiciel explose.

**Le Sauveur (Pivotage Partiel $PA = LU$) :**
L'algorithme moderne ne fonce pas t√™te baiss√©e. √Ä chaque colonne de travail, il regarde vers le bas toutes les lignes, s√©lectionne **la plus grande valeur absolue**, et √©change g√©om√©triquement l'ordre de TOUTE la ligne de la matrice. L'ordinateur m√©morise cet √©change dans une matrice de permutation unitaire modifi√©e $P$.

L'algorithme parfait et inconditionnellement stable de l'Analyse Num√©rique n'est donc pas $A=LU$, mais **$PA = LU$**.
</details>

---

**Exercice 20 ‚Äî L'Astuce de $\det(A)$**

En vous appuyant du th√©or√®me des d√©terminants $\det(AB) = \det(A)\det(B)$, et de la sp√©cificit√© de la diagonale de $L, U$ et $P$; d√©montrez pourquoi Octave/Matlab peut calculer le d√©terminant d'une matrice gigantesque instantan√©ment d√®s que $PA = LU$ est termin√©.

<details>
<summary>Voir la r√©ponse et l'explication d√©taill√©e</summary>

Prenons la base : $PA = LU$
$$
\det(PA) = \det(LU) \implies \det(P)\det(A) = \det(L)\det(U)
$$

Regardons les propri√©t√©s incroyables des actants :
- **$P$** : C'est une matrice d'Identit√© o√π les lignes ont √©t√© interchang√©es $p$ fois. Donc $\det(P) = (-1)^p$.
- **$L$** : Matrice triangulaire. Son d√©terminant est produit de sa diagonale. Son principe d'existence exige des $1$ absolus sur sa diagonale. Donc $\det(L) = 1$.
- **$U$** : Matrice triangulaire. Son d√©terminant est aussi le produit de sa diagonale (les pivots ultimes : $u_{11}, u_{22}, \dots, u_{nn}$).

Injectons tout √ßa dans l'√©quation :
$$
(-1)^p \cdot \det(A) = 1 \cdot \prod u_{ii} \implies \det(A) = (-1)^p \cdot (u_{11} \cdot u_{22} \cdots u_{nn})
$$

**Conclusion magique :** D√®s que Gauss a ex√©cut√© son $PA = LU$ (qui de toutes fa√ßons est n√©cessaire pour plein d'autres choses), le d√©terminant complet du syst√®me originel se r√©v√®le par simple multiplication de l'ossature diagonale de $U$ !
</details>
